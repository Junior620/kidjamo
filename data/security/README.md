# Modules de S√©curit√© - Pseudonymisation et Chiffrement

[![Security](https://img.shields.io/badge/Security-RGPD%2FHIPAA-green.svg)](https://gdpr.eu)
[![Crypto](https://img.shields.io/badge/Crypto-HMAC%20SHA256-blue.svg)](https://tools.ietf.org/html/rfc2104)
[![PySpark](https://img.shields.io/badge/PySpark-DataFrame-orange.svg)](https://spark.apache.org)
[![Audit](https://img.shields.io/badge/Audit-Trail-purple.svg)](https://en.wikipedia.org/wiki/Audit_trail)

## üîí Architecture de S√©curit√©

```mermaid
graph TB
    subgraph "Zone D√©veloppement"
        A[DataFrame Patient ID] --> B[pseudonymization.py]
        B --> C[SHA256 + Salt Hardcod√©]
        C --> D[Patient ID Pseudonymis√©]
    end
    
    subgraph "Zone Production"
        E[Patient ID Sensible] --> F[pseudonymization_secure.py]
        F --> G[HMAC-SHA256 + Salt .env]
        G --> H[Patient ID S√©curis√©]
        F --> I[Audit Trail JSONL]
        F --> J[Rotation Cl√©s]
    end
    
    subgraph "Donn√©es Prot√©g√©es"
        K[PII: patient_id, device_id]
        L[Clinique: vitaux, diagnostics]
        M[Chiffrement Fernet]
    end
    
    style A fill:#ffeb3b
    style E fill:#f44336
    style H fill:#4caf50
    style I fill:#2196f3
```

## üìÇ Structure des Modules

### 1. **pseudonymization.py** - Transformations Spark (Dev/Bronze/Silver)
```
üéØ R√¥le : Pseudonymisation c√¥t√© PySpark pour analyses
üì• Entr√©es : DataFrame Spark avec patient_id en clair
üì§ Sorties : DataFrame avec patient_id SHA256 + salt
üîß Usage : Zones analytiques o√π l'ID original n'est plus n√©cessaire
‚ö†Ô∏è  Limite : Salt hardcod√© visible (d√©veloppement uniquement)
```

### 2. **pseudonymization_secure.py** - Production HMAC + Audit
```
üéØ R√¥le : S√©curit√© production avec .env et audit trail
üì• Entr√©es : Variables .env + patient_id √† pseudonymiser
üì§ Sorties : HMAC-SHA256 + logs audit + rotation cl√©s
üîß Usage : Production avec conformit√© RGPD/HIPAA
‚úÖ S√©curit√© : Salt depuis .env, HMAC r√©sistant, audit complet
```

## üöÄ Guide d'Utilisation

### Configuration Environnement (.env)

Cr√©ez un fichier `.env` √† la racine du projet :

```bash
# Cl√© HMAC pour pseudonymisation (g√©n√©rer avec: python -c "import secrets; print(secrets.token_hex(32))")
PSEUDONYMIZATION_SALT=a1b2c3d4e5f6...64_chars_hex

# Cl√© Fernet pour chiffrement (g√©n√©rer avec: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
ENCRYPTION_KEY=your-fernet-key-base64==

# Rotation des cl√©s (optionnel, d√©faut: 24h)
KEY_ROTATION_INTERVAL_HOURS=24

# R√©pertoire d'audit (optionnel, d√©faut: evidence)
EVIDENCE_PATH=evidence
```

### G√©n√©ration des Cl√©s S√©curis√©es

```python
# G√©n√©rer PSEUDONYMIZATION_SALT
import secrets
salt = secrets.token_hex(32)  # 64 caract√®res hex
print(f"PSEUDONYMIZATION_SALT={salt}")

# G√©n√©rer ENCRYPTION_KEY
from cryptography.fernet import Fernet
key = Fernet.generate_key()
print(f"ENCRYPTION_KEY={key.decode()}")
```

## üíª Exemples d'Utilisation

### 1. **Pseudonymisation Spark (D√©veloppement)**

```python
from pyspark.sql import SparkSession
from data.security.pseudonymization import pseudonymize_patient_data

# Initialisation Spark
spark = SparkSession.builder.appName("SecurityExample").getOrCreate()

# DataFrame avec IDs patients
df_patients = spark.createDataFrame([
    ("PAT-001", "John Doe", 120),
    ("PAT-002", "Jane Smith", 85),
    ("PAT-003", "Bob Wilson", 95)
], ["patient_id", "name", "heart_rate"])

# Pseudonymisation avec salt par d√©faut
df_pseudo = pseudonymize_patient_data(df_patients)

df_pseudo.show()
# +------------------------------------------------------------+----------+----------+
# |patient_id                                                  |name      |heart_rate|
# +------------------------------------------------------------+----------+----------+
# |a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef123456|John Doe  |120       |
# |b2c3d4e5f67890a1bcdef234567890a1bcdef234567890a1bcdef234567|Jane Smith|85        |
# |c3d4e5f67890a1b2cdef34567890a1b2cdef34567890a1b2cdef345678|Bob Wilson|95        |
# +------------------------------------------------------------+----------+----------+

# Pseudonymisation avec salt personnalis√©
df_custom = pseudonymize_patient_data(df_patients, salt="custom_salt_2025")
```

### 2. **Pseudonymisation S√©curis√©e (Production)**

```python
from data.security.pseudonymization_secure import hash_patient_id, secure_pseudonymizer

# Pseudonymisation via fonction wrapper (compatibilit√©)
patient_hash = hash_patient_id("PAT-12345")
print(f"Hash: {patient_hash}")
# Output: a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890ab

# Pseudonymisation via instance (acc√®s complet)
try:
    patient_hash = secure_pseudonymizer.hash_patient_id("PAT-12345")
    print(f"Secure Hash: {patient_hash}")
except ValueError as e:
    print(f"Erreur configuration: {e}")
    # V√©rifiez que .env contient PSEUDONYMIZATION_SALT et ENCRYPTION_KEY
```

### 3. **Classification des Donn√©es**

```python
from data.security.pseudonymization import apply_data_classification

# DataFrame avec colonnes sensibles
df_medical = spark.createDataFrame([
    ("PAT-001", "DEV-001", 120, 98.5, 36.8),
    ("PAT-002", "DEV-002", 85, 97.2, 37.1)
], ["patient_id", "device_id", "heart_rate_bpm", "SpO2", "temperature_core_c"])

# Classification des donn√©es (inventaire actuel)
df_classified = apply_data_classification(df_medical)

# Future extension: masquage automatique selon classification
# df_masked = apply_data_classification(df_medical, apply_masking=True)
```

## üîç Monitoring et Audit

### Logs de S√©curit√©

Le module s√©curis√© g√©n√®re automatiquement des logs d'audit :

```bash
# Fichier: evidence/security_audit.log (format JSONL)
{"timestamp": "2025-08-24T10:30:00.123456", "event_type": "patient_id_hashed", "details": {"original_length": 7, "hash_prefix": "a1b2c3d4...", "algorithm": "HMAC-SHA256"}}
{"timestamp": "2025-08-24T10:30:15.789012", "event_type": "key_rotation_check", "details": {"hours_since_rotation": 2.5, "rotation_needed": false}}
```

### M√©tadonn√©es de Rotation

```bash
# Fichier: evidence/last_key_rotation.json
{
  "last_rotation": "2025-08-24T08:00:00.000000",
  "rotation_interval_hours": 24
}
```

### Analyse des Logs

```python
import json
from datetime import datetime

# Lecture des logs d'audit
with open('evidence/security_audit.log', 'r') as f:
    for line in f:
        event = json.loads(line)
        print(f"{event['timestamp']}: {event['event_type']}")
        if event['event_type'] == 'patient_id_hashed':
            print(f"  Hash g√©n√©r√©: {event['details']['hash_prefix']}")
```

## üõ°Ô∏è S√©curit√© et Conformit√©

### Standards Respect√©s

- **RGPD Article 4(5)** : Pseudonymisation des donn√©es personnelles
- **HIPAA Safe Harbor** : Suppression des 18 identifiants directs
- **NIST Cybersecurity Framework** : Protect (PR.DS) - Data Security
- **ISO 27001** : Contr√¥les cryptographiques et audit trail

### Menaces Couvertes

| Menace | Module Dev | Module Secure | Mitigation |
|--------|------------|---------------|------------|
| **Exposition Salt** | ‚ùå Hardcod√© | ‚úÖ .env | Variables d'environnement |
| **Attaque Extension** | ‚ö†Ô∏è SHA256 simple | ‚úÖ HMAC-SHA256 | R√©sistance cryptographique |
| **Cl√©s Compromises** | ‚ùå Statique | ‚úÖ Rotation | Rotation automatique 24h |
| **Perte Tra√ßabilit√©** | ‚ùå Aucun log | ‚úÖ Audit JSONL | Logs append-only |
| **Non-conformit√©** | ‚ö†Ô∏è Partielle | ‚úÖ RGPD/HIPAA | Audit trail complet |

### Limites Connues

‚ö†Ô∏è **Pseudonymisation ‚â† Anonymisation**
- Re-identification possible avec table de correspondance
- Combinaison avec autres datasets peut r√©v√©ler l'identit√©
- Pour anonymisation : k-anonymity, l-diversity, t-closeness

‚ö†Ô∏è **Cl√©s en M√©moire**
- Production critique : consid√©rer HSM (Hardware Security Module)
- Vault management pour rotation automatique
- Chiffrement at-rest pour logs d'audit

## üîß Configuration Avanc√©e

### Int√©gration avec Spark Streaming

```python
from pyspark.sql import functions as F
from data.security.pseudonymization import pseudonymize_patient_data

# Stream Kafka avec pseudonymisation
df_stream = (spark
    .readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "patient_data")
    .load())

# Parse JSON et pseudonymise
df_parsed = (df_stream
    .select(F.from_json(F.col("value").cast("string"), schema).alias("data"))
    .select("data.*"))

# Pseudonymisation sur stream
df_secure_stream = pseudonymize_patient_data(df_parsed)

# √âcriture s√©curis√©e
query = (df_secure_stream
    .writeStream
    .format("parquet")
    .option("path", "hdfs://secure_lake/bronze/")
    .option("checkpointLocation", "hdfs://checkpoints/secure/")
    .start())
```

### Pipeline ETL S√©curis√©e

```python
# Exemple pipeline Bronze ‚Üí Silver avec pseudonymisation
def secure_etl_pipeline(input_path: str, output_path: str):
    """Pipeline ETL avec pseudonymisation automatique."""
    
    # Lecture donn√©es Bronze
    df_bronze = spark.read.parquet(input_path)
    
    # Classification et inventaire
    df_classified = apply_data_classification(df_bronze)
    
    # Pseudonymisation des IDs patients
    df_pseudo = pseudonymize_patient_data(df_classified)
    
    # √âcriture Silver s√©curis√©e
    (df_pseudo
        .write
        .mode("overwrite")
        .option("encryption", "sse-s3")
        .parquet(output_path))
    
    print(f"Pipeline s√©curis√©: {input_path} ‚Üí {output_path}")

# Utilisation
secure_etl_pipeline(
    "s3a://data-lake/bronze/iot_measurements/",
    "s3a://data-lake/silver/iot_measurements_pseudo/"
)
```

### Tests de S√©curit√©

```python
import pytest
from data.security.pseudonymization import pseudonymize_patient_data
from data.security.pseudonymization_secure import hash_patient_id

def test_pseudonymization_deterministic():
    """V√©rifie que la pseudonymisation est d√©terministe."""
    patient_id = "PAT-TEST-001"
    
    # M√™me input doit donner m√™me output
    hash1 = hash_patient_id(patient_id)
    hash2 = hash_patient_id(patient_id)
    
    assert hash1 == hash2
    assert len(hash1) == 64  # SHA256 hex = 64 chars
    
def test_pseudonymization_different_salts():
    """V√©rifie que diff√©rents salts donnent diff√©rents hashs."""
    df = spark.createDataFrame([("PAT-001",)], ["patient_id"])
    
    df1 = pseudonymize_patient_data(df, salt="salt1")
    df2 = pseudonymize_patient_data(df, salt="salt2")
    
    hash1 = df1.collect()[0]["patient_id"]
    hash2 = df2.collect()[0]["patient_id"]
    
    assert hash1 != hash2

def test_security_audit_log():
    """V√©rifie que les logs d'audit sont g√©n√©r√©s."""
    import os
    from data.security.pseudonymization_secure import secure_pseudonymizer
    
    # Hash avec audit
    patient_hash = secure_pseudonymizer.hash_patient_id("PAT-AUDIT-TEST")
    
    # V√©rifier log cr√©√©
    assert os.path.exists(secure_pseudonymizer.audit_log_path)
    
    with open(secure_pseudonymizer.audit_log_path, 'r') as f:
        logs = f.readlines()
        assert len(logs) > 0
        last_log = json.loads(logs[-1])
        assert last_log['event_type'] == 'patient_id_hashed'
```

## üìä Performance et Optimisation

### Benchmarks

```python
import time
from data.security.pseudonymization_secure import hash_patient_id

# Test performance HMAC vs SHA256 simple
def benchmark_hashing(n_iterations=10000):
    patient_ids = [f"PAT-{i:06d}" for i in range(n_iterations)]
    
    start_time = time.time()
    for pid in patient_ids:
        hash_patient_id(pid)
    end_time = time.time()
    
    total_time = end_time - start_time
    print(f"HMAC-SHA256: {n_iterations} hash en {total_time:.3f}s")
    print(f"Performance: {n_iterations/total_time:.0f} hash/seconde")

# R√©sultats attendus:
# HMAC-SHA256: 10000 hash en 0.245s
# Performance: 40816 hash/seconde
```

### Optimisation Spark

```python
# Optimisation pour gros datasets
def optimized_pseudonymize(df, batch_size=10000):
    """Pseudonymisation optimis√©e par batch."""
    
    # Partitioning par patient_id pour parall√©lisme
    df_partitioned = df.repartition("patient_id")
    
    # Cache pour √©viter recalculs
    df_cached = df_partitioned.cache()
    
    # Pseudonymisation avec broadcast du salt
    salt_broadcast = spark.sparkContext.broadcast("kidjamo_salt_2025")
    
    df_pseudo = pseudonymize_patient_data(df_cached, salt_broadcast.value)
    
    return df_pseudo
```

## üîÑ Migration et Compatibilit√©

### Migration Dev ‚Üí Production

```python
# Script de migration des donn√©es pseudonymis√©es
def migrate_to_secure_hashing():
    """Migre les hash SHA256 simple vers HMAC-SHA256."""
    
    # Lecture des donn√©es avec anciens hashs
    df_old = spark.read.parquet("s3://lake/silver/old_pseudo/")
    
    # Re-pseudonymisation avec module s√©curis√©
    # Note: n√©cessite les IDs originaux (table de correspondance)
    df_secure = pseudonymize_patient_data(df_original, salt=secure_salt)
    
    # √âcriture avec nouvelle structure
    df_secure.write.mode("overwrite").parquet("s3://lake/silver/secure_pseudo/")
```

### Compatibilit√© Descendante

```python
# Wrapper pour anciens appels
def legacy_hash_patient_id(patient_id: str, use_secure: bool = True) -> str:
    """Wrapper compatibilit√© pour ancienne API."""
    if use_secure:
        from data.security.pseudonymization_secure import hash_patient_id
        return hash_patient_id(patient_id)
    else:
        # Fallback ancien comportement (dev uniquement)
        import hashlib
        return hashlib.sha256(f"{patient_id}kidjamo_salt_2025".encode()).hexdigest()
```

## üö® D√©pannage

### Probl√®mes Courants

#### 1. **Erreur Variables .env Manquantes**
```bash
# Erreur: PSEUDONYMIZATION_SALT non d√©fini dans .env
# Solution: Cr√©er .env avec cl√©s g√©n√©r√©es
echo "PSEUDONYMIZATION_SALT=$(python -c 'import secrets; print(secrets.token_hex(32))')" >> .env
echo "ENCRYPTION_KEY=$(python -c 'from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())')" >> .env
```

#### 2. **Logs d'Audit Volumineux**
```bash
# Rotation manuelle des logs (√† automatiser)
mv evidence/security_audit.log evidence/security_audit_$(date +%Y%m%d).log
touch evidence/security_audit.log
```

#### 3. **Performance Spark Lente**
```python
# Optimisation configuration Spark
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
```

## üìö Documentation Avanc√©e

### API Reference

#### pseudonymization.py
- `pseudonymize_patient_data(df, salt)` ‚Üí DataFrame pseudonymis√©
- `apply_data_classification(df)` ‚Üí DataFrame classifi√©
- `DEFAULT_SALT` ‚Üí "kidjamo_salt_2025"
- `PII_COLUMNS` ‚Üí Liste colonnes identifiantes
- `CLINICAL_COLUMNS` ‚Üí Liste colonnes m√©dicales

#### pseudonymization_secure.py
- `SecurePseudonymizer()` ‚Üí Instance gestionnaire s√©curis√©
- `hash_patient_id(patient_id)` ‚Üí Hash HMAC-SHA256
- `secure_pseudonymizer` ‚Üí Instance globale
- Variables .env : `PSEUDONYMIZATION_SALT`, `ENCRYPTION_KEY`

### Standards de Codage

- **PEP8** : Style et formatage respect√©s
- **Type Hints** : Annotations sur fonctions publiques
- **Docstrings** : Format Google avec exemples
- **Logging** : Audit trail structur√© JSON
- **Tests** : Couverture s√©curit√© et performance

## üéØ Roadmap

### Am√©liorations Futures
- [ ] **HSM Integration** : Hardware Security Module pour cl√©s critiques
- [ ] **Vault Management** : Rotation automatique via HashiCorp Vault
- [ ] **Anonymisation K-anonymity** : Techniques avanc√©es d'anonymisation
- [ ] **Chiffrement at-rest** : Logs d'audit chiffr√©s
- [ ] **API Governance** : Politiques d'acc√®s par classification
- [ ] **ML Privacy** : Differential privacy pour mod√®les

---

## ü§ù Contribution

### Standards S√©curit√©
- Code review obligatoire pour changements cryptographiques
- Tests de s√©curit√© syst√©matiques
- Audit trail pr√©serv√© en toutes circonstances
- Documentation mise √† jour avec exemples

### Process de Validation
1. Tests unitaires s√©curit√© passants
2. Benchmark performance (aucune r√©gression)
3. Validation conformit√© RGPD/HIPAA
4. Review cryptographique par expert s√©curit√©

---

**üîê Contact S√©curit√© :** security@kidjamo.com  
**üìñ Documentation :** [docs/security/](../../docs/security/)  
**üö® Incidents :** [security-incidents@kidjamo.com](mailto:security-incidents@kidjamo.com)
