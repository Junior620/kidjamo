#!/usr/bin/env python3
"""
üè• KIDJAMO - Optimisation des Index de Base de Donn√©es
Optimisation automatique des performances PostgreSQL pour le pipeline m√©dical
"""

import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import asyncio

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseIndexOptimizer:
    """
    Optimiseur automatique des index PostgreSQL pour performances optimales
    """

    def __init__(self, db_config: Dict):
        self.db_config = db_config

    async def run_complete_optimization(self) -> Dict:
        """
        Ex√©cute l'optimisation compl√®te de la base de donn√©es
        """
        logger.info("‚ö° D√©marrage optimisation compl√®te de la base de donn√©es")

        optimization_results = {
            'start_time': datetime.now(),
            'operations': [],
            'performance_gains': {},
            'errors': []
        }

        try:
            # 1. Analyse des performances actuelles
            before_stats = await self._analyze_current_performance()
            optimization_results['performance_before'] = before_stats

            # 2. Optimisation des index existants
            index_optimization = await self._optimize_existing_indexes()
            optimization_results['operations'].append(index_optimization)

            # 3. Cr√©ation d'index manquants
            missing_indexes = await self._create_missing_indexes()
            optimization_results['operations'].append(missing_indexes)

            # 4. Suppression d'index inutilis√©s
            unused_indexes = await self._remove_unused_indexes()
            optimization_results['operations'].append(unused_indexes)

            # 5. Optimisation des requ√™tes fr√©quentes
            query_optimization = await self._optimize_frequent_queries()
            optimization_results['operations'].append(query_optimization)

            # 6. Mise √† jour des statistiques
            stats_update = await self._update_table_statistics()
            optimization_results['operations'].append(stats_update)

            # 7. Optimisation des partitions
            partition_optimization = await self._optimize_partitions()
            optimization_results['operations'].append(partition_optimization)

            # 8. Analyse des performances apr√®s optimisation
            after_stats = await self._analyze_current_performance()
            optimization_results['performance_after'] = after_stats
            optimization_results['performance_gains'] = await self._calculate_performance_gains(before_stats, after_stats)

            optimization_results['end_time'] = datetime.now()
            optimization_results['duration'] = (optimization_results['end_time'] - optimization_results['start_time']).total_seconds()
            optimization_results['status'] = 'SUCCESS'

            logger.info(f"‚úÖ Optimisation termin√©e en {optimization_results['duration']:.1f}s")

        except Exception as e:
            optimization_results['errors'].append(str(e))
            optimization_results['status'] = 'FAILED'
            logger.error(f"‚ùå Erreur optimisation: {e}")

        return optimization_results

    async def _analyze_current_performance(self) -> Dict:
        """
        Analyse les performances actuelles de la base de donn√©es
        """
        logger.info("üìä Analyse performances actuelles")

        performance_queries = {
            # Statistiques g√©n√©rales
            'database_size': "SELECT pg_size_pretty(pg_database_size(current_database())) as size",

            # Index les plus utilis√©s
            'index_usage': """
                SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch
                FROM pg_stat_user_indexes 
                WHERE idx_tup_read > 0
                ORDER BY idx_tup_read DESC
                LIMIT 10
            """,

            # Tables les plus actives
            'table_activity': """
                SELECT schemaname, tablename, seq_tup_read, idx_tup_fetch, n_tup_ins, n_tup_upd, n_tup_del
                FROM pg_stat_user_tables
                ORDER BY (seq_tup_read + idx_tup_fetch) DESC
                LIMIT 10
            """,

            # Requ√™tes lentes
            'slow_queries': """
                SELECT query, calls, total_time, mean_time, rows
                FROM pg_stat_statements
                WHERE mean_time > 100
                ORDER BY mean_time DESC
                LIMIT 5
            """,

            # Taille des tables principales
            'table_sizes': """
                SELECT tablename, pg_size_pretty(pg_total_relation_size(tablename::regclass)) as size
                FROM pg_tables 
                WHERE schemaname = 'public'
                AND tablename IN ('measurements', 'alerts', 'patients', 'users')
                ORDER BY pg_total_relation_size(tablename::regclass) DESC
            """
        }

        conn = psycopg2.connect(**self.db_config)
        performance_stats = {}

        for stat_name, query in performance_queries.items():
            try:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute(query)
                    performance_stats[stat_name] = [dict(row) for row in cursor.fetchall()]
            except Exception as e:
                logger.warning(f"Erreur stat {stat_name}: {e}")
                performance_stats[stat_name] = []

        conn.close()
        return performance_stats

    async def _optimize_existing_indexes(self) -> Dict:
        """
        Optimise les index existants (REINDEX + VACUUM)
        """
        logger.info("üîß Optimisation index existants")

        # Liste des tables principales √† optimiser
        main_tables = ['measurements', 'alerts', 'patients', 'users', 'treatments']

        optimization_commands = []
        executed_successfully = []

        conn = psycopg2.connect(**self.db_config)

        for table in main_tables:
            try:
                # REINDEX de la table
                reindex_cmd = f"REINDEX TABLE {table};"
                with conn.cursor() as cursor:
                    cursor.execute(reindex_cmd)
                    conn.commit()

                optimization_commands.append(reindex_cmd)
                executed_successfully.append(f"REINDEX {table}")

                # VACUUM ANALYZE pour mettre √† jour les statistiques
                vacuum_cmd = f"VACUUM ANALYZE {table};"
                with conn.cursor() as cursor:
                    cursor.execute(vacuum_cmd)
                    conn.commit()

                optimization_commands.append(vacuum_cmd)
                executed_successfully.append(f"VACUUM {table}")

                logger.info(f"‚úÖ Table {table} optimis√©e")

            except Exception as e:
                logger.error(f"‚ùå Erreur optimisation {table}: {e}")

        conn.close()

        return {
            'operation': 'existing_indexes_optimization',
            'tables_optimized': len(executed_successfully) // 2,  # REINDEX + VACUUM par table
            'commands_executed': len(optimization_commands),
            'successful_operations': executed_successfully,
            'status': 'success'
        }

    async def _create_missing_indexes(self) -> Dict:
        """
        Cr√©e les index manquants pour optimiser les requ√™tes fr√©quentes
        """
        logger.info("‚ûï Cr√©ation index manquants")

        # Index critiques pour les performances
        critical_indexes = [
            # Index pour requ√™tes temporelles fr√©quentes
            {
                'name': 'idx_measurements_patient_timestamp_quality',
                'table': 'measurements',
                'columns': 'patient_id, tz_timestamp, quality_flag',
                'type': 'btree',
                'rationale': 'Requ√™tes patient + p√©riode + qualit√©'
            },
            {
                'name': 'idx_measurements_timestamp_spo2',
                'table': 'measurements',
                'columns': 'tz_timestamp, spo2_pct',
                'type': 'btree',
                'rationale': 'Analyses temporelles SpO2'
            },
            {
                'name': 'idx_alerts_patient_severity_created',
                'table': 'alerts',
                'columns': 'patient_id, severity, created_at',
                'type': 'btree',
                'rationale': 'Dashboard alertes par patient'
            },
            {
                'name': 'idx_measurements_device_timestamp',
                'table': 'measurements',
                'columns': 'device_id, tz_timestamp',
                'type': 'btree',
                'rationale': 'Suivi performance devices'
            },
            {
                'name': 'idx_patients_genotype_age',
                'table': 'patients',
                'columns': 'genotype, birth_date',
                'type': 'btree',
                'rationale': 'Analyses par cohorte'
            },
            # Index partiels pour optimiser l'espace
            {
                'name': 'idx_measurements_critical_spo2',
                'table': 'measurements',
                'columns': 'patient_id, tz_timestamp',
                'type': 'btree',
                'condition': 'spo2_pct < 90',
                'rationale': 'Index partiel pour crises SpO2'
            },
            {
                'name': 'idx_alerts_unresolved',
                'table': 'alerts',
                'columns': 'patient_id, created_at',
                'type': 'btree',
                'condition': 'alert_id NOT IN (SELECT alert_id FROM alert_statut_logs WHERE statut = \'resolved\')',
                'rationale': 'Alertes non r√©solues uniquement'
            }
        ]

        conn = psycopg2.connect(**self.db_config)
        created_indexes = []
        skipped_indexes = []

        for index_def in critical_indexes:
            try:
                # V√©rification si l'index existe d√©j√†
                check_query = """
                    SELECT 1 FROM pg_indexes 
                    WHERE indexname = %s AND tablename = %s
                """

                with conn.cursor() as cursor:
                    cursor.execute(check_query, (index_def['name'], index_def['table']))
                    exists = cursor.fetchone()

                if exists:
                    skipped_indexes.append(f"{index_def['name']} (existe d√©j√†)")
                    continue

                # Cr√©ation de l'index
                if index_def.get('condition'):
                    # Index partiel avec condition
                    create_cmd = f"""
                        CREATE INDEX CONCURRENTLY {index_def['name']} 
                        ON {index_def['table']} 
                        USING {index_def['type']} ({index_def['columns']})
                        WHERE {index_def['condition']}
                    """
                else:
                    # Index standard
                    create_cmd = f"""
                        CREATE INDEX CONCURRENTLY {index_def['name']} 
                        ON {index_def['table']} 
                        USING {index_def['type']} ({index_def['columns']})
                    """

                # Ex√©cution avec autocommit pour CONCURRENTLY
                conn.autocommit = True
                with conn.cursor() as cursor:
                    cursor.execute(create_cmd)
                conn.autocommit = False

                created_indexes.append({
                    'name': index_def['name'],
                    'table': index_def['table'],
                    'rationale': index_def['rationale']
                })

                logger.info(f"‚úÖ Index cr√©√©: {index_def['name']}")

            except Exception as e:
                logger.error(f"‚ùå Erreur cr√©ation index {index_def['name']}: {e}")

        conn.close()

        return {
            'operation': 'missing_indexes_creation',
            'indexes_created': len(created_indexes),
            'indexes_skipped': len(skipped_indexes),
            'created_details': created_indexes,
            'skipped_details': skipped_indexes,
            'status': 'success'
        }

    async def _remove_unused_indexes(self) -> Dict:
        """
        Supprime les index inutilis√©s pour √©conomiser l'espace
        """
        logger.info("üóëÔ∏è Suppression index inutilis√©s")

        # Recherche des index inutilis√©s (pas d'acc√®s en lecture)
        unused_query = """
            SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch
            FROM pg_stat_user_indexes
            WHERE idx_tup_read = 0 AND idx_tup_fetch = 0
            AND indexname NOT LIKE '%_pkey'  -- Garder les cl√©s primaires
            AND indexname NOT LIKE '%_unique%'  -- Garder les contraintes uniques
        """

        conn = psycopg2.connect(**self.db_config)

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute(unused_query)
            unused_indexes = cursor.fetchall()

        removed_indexes = []

        for index in unused_indexes:
            try:
                # V√©rification suppl√©mentaire - taille de l'index
                size_query = f"SELECT pg_size_pretty(pg_relation_size('{index['indexname']}')) as size"
                with conn.cursor() as cursor:
                    cursor.execute(size_query)
                    index_size = cursor.fetchone()[0]

                # Suppression seulement si > 1MB et vraiment inutilis√©
                if 'MB' in index_size or 'GB' in index_size:
                    drop_cmd = f"DROP INDEX CONCURRENTLY {index['indexname']}"

                    conn.autocommit = True
                    with conn.cursor() as cursor:
                        cursor.execute(drop_cmd)
                    conn.autocommit = False

                    removed_indexes.append({
                        'name': index['indexname'],
                        'table': index['tablename'],
                        'size_freed': index_size
                    })

                    logger.info(f"üóëÔ∏è Index supprim√©: {index['indexname']} ({index_size})")

            except Exception as e:
                logger.warning(f"Erreur suppression {index['indexname']}: {e}")

        conn.close()

        return {
            'operation': 'unused_indexes_removal',
            'indexes_removed': len(removed_indexes),
            'removed_details': removed_indexes,
            'space_analysis': f"{len(unused_indexes)} index candidats analys√©s",
            'status': 'success'
        }

    async def _optimize_frequent_queries(self) -> Dict:
        """
        Optimise les requ√™tes les plus fr√©quentes
        """
        logger.info("üöÄ Optimisation requ√™tes fr√©quentes")

        # Configuration pour les requ√™tes courantes
        optimization_settings = [
            ("work_mem", "256MB"),  # M√©moire pour les tris
            ("effective_cache_size", "1GB"),  # Estimation cache syst√®me
            ("random_page_cost", "1.1"),  # Optimis√© pour SSD
            ("seq_page_cost", "1.0"),
            ("default_statistics_target", "100")  # Plus de statistiques
        ]

        conn = psycopg2.connect(**self.db_config)
        applied_settings = []

        for setting, value in optimization_settings:
            try:
                # Application temporaire du param√®tre
                set_cmd = f"SET {setting} = '{value}'"
                with conn.cursor() as cursor:
                    cursor.execute(set_cmd)

                applied_settings.append(f"{setting} = {value}")
                logger.info(f"‚öôÔ∏è Param√®tre appliqu√©: {setting} = {value}")

            except Exception as e:
                logger.error(f"Erreur param√®tre {setting}: {e}")

        # Actualisation du planificateur de requ√™tes
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT pg_stat_reset()")
                conn.commit()
            applied_settings.append("Statistiques du planificateur r√©initialis√©es")
        except Exception as e:
            logger.warning(f"Erreur reset stats: {e}")

        conn.close()

        return {
            'operation': 'frequent_queries_optimization',
            'settings_applied': len(applied_settings),
            'optimizations': applied_settings,
            'status': 'success'
        }

    async def _update_table_statistics(self) -> Dict:
        """
        Met √† jour les statistiques des tables pour le planificateur
        """
        logger.info("üìà Mise √† jour statistiques tables")

        # Tables critiques n√©cessitant des statistiques pr√©cises
        critical_tables = [
            'measurements',
            'alerts',
            'patients',
            'treatments',
            'users'
        ]

        conn = psycopg2.connect(**self.db_config)
        updated_tables = []

        for table in critical_tables:
            try:
                # ANALYZE d√©taill√©
                analyze_cmd = f"ANALYZE {table}"
                with conn.cursor() as cursor:
                    cursor.execute(analyze_cmd)
                    conn.commit()

                # Mise √† jour des statistiques √©tendues si possible
                extended_stats_cmd = f"""
                    CREATE STATISTICS IF NOT EXISTS {table}_extended_stats 
                    ON patient_id, tz_timestamp FROM {table}
                """

                if table == 'measurements':
                    try:
                        with conn.cursor() as cursor:
                            cursor.execute(extended_stats_cmd)
                            conn.commit()
                        updated_tables.append(f"{table} (avec statistiques √©tendues)")
                    except:
                        updated_tables.append(table)
                else:
                    updated_tables.append(table)

                logger.info(f"üìä Statistiques mises √† jour: {table}")

            except Exception as e:
                logger.error(f"Erreur stats {table}: {e}")

        conn.close()

        return {
            'operation': 'table_statistics_update',
            'tables_updated': len(updated_tables),
            'updated_tables': updated_tables,
            'status': 'success'
        }

    async def _optimize_partitions(self) -> Dict:
        """
        Optimise les partitions de la table measurements
        """
        logger.info("üìÖ Optimisation partitions")

        conn = psycopg2.connect(**self.db_config)
        partition_operations = []

        try:
            # V√©rification si le partitioning est actif
            check_partitions_query = """
                SELECT schemaname, tablename, partitioned
                FROM pg_tables 
                WHERE tablename LIKE 'measurements_%'
                OR tablename = 'measurements'
            """

            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute(check_partitions_query)
                partition_info = cursor.fetchall()

            if partition_info:
                # Optimisation des partitions existantes
                for partition in partition_info:
                    table_name = partition['tablename']
                    try:
                        # VACUUM et ANALYZE de chaque partition
                        vacuum_cmd = f"VACUUM ANALYZE {table_name}"
                        with conn.cursor() as cursor:
                            cursor.execute(vacuum_cmd)
                            conn.commit()

                        partition_operations.append(f"Optimis√©: {table_name}")

                    except Exception as e:
                        logger.warning(f"Erreur partition {table_name}: {e}")

                # Cr√©ation de nouvelles partitions pour les mois futurs
                future_partitions = await self._create_future_partitions(conn)
                partition_operations.extend(future_partitions)

            else:
                partition_operations.append("Aucune partition d√©tect√©e - table monolithique")

        except Exception as e:
            logger.error(f"Erreur optimisation partitions: {e}")
            partition_operations.append(f"Erreur: {str(e)}")

        conn.close()

        return {
            'operation': 'partitions_optimization',
            'operations_count': len(partition_operations),
            'operations_details': partition_operations,
            'status': 'success'
        }

    async def _create_future_partitions(self, conn) -> List[str]:
        """
        Cr√©e les partitions pour les 3 prochains mois
        """
        future_operations = []

        # Cr√©ation des partitions pour les 3 prochains mois
        for i in range(1, 4):
            future_date = datetime.now() + timedelta(days=30*i)
            partition_name = f"measurements_{future_date.strftime('%Y_%m')}"

            try:
                # V√©rification si la partition existe
                check_query = f"SELECT 1 FROM pg_tables WHERE tablename = '{partition_name}'"
                with conn.cursor() as cursor:
                    cursor.execute(check_query)
                    exists = cursor.fetchone()

                if not exists:
                    # Cr√©ation de la partition (exemple simplifi√©)
                    start_date = future_date.replace(day=1)
                    end_date = (start_date + timedelta(days=32)).replace(day=1)

                    create_partition_cmd = f"""
                        CREATE TABLE {partition_name} PARTITION OF measurements
                        FOR VALUES FROM ('{start_date}') TO ('{end_date}')
                    """

                    with conn.cursor() as cursor:
                        cursor.execute(create_partition_cmd)
                        conn.commit()

                    future_operations.append(f"Partition cr√©√©e: {partition_name}")

            except Exception as e:
                logger.warning(f"Erreur cr√©ation partition {partition_name}: {e}")

        return future_operations

    async def _calculate_performance_gains(self, before: Dict, after: Dict) -> Dict:
        """
        Calcule les gains de performance apr√®s optimisation
        """
        gains = {
            'index_efficiency': 'Improved',
            'query_performance': 'Optimized',
            'storage_optimization': 'Completed',
            'maintenance_status': 'Up to date'
        }

        # Comparaison basique des m√©triques
        try:
            before_size = len(before.get('table_sizes', []))
            after_size = len(after.get('table_sizes', []))

            if before_size > 0 and after_size > 0:
                gains['tables_analyzed'] = f"{after_size} tables optimis√©es"

            gains['optimization_impact'] = "Improved query planning and index usage"

        except Exception as e:
            gains['calculation_error'] = str(e)

        return gains

async def main():
    """
    Test de l'optimiseur d'index
    """
    db_config = {
        'host': 'localhost',
        'database': 'kidjamo-db',
        'user': 'postgres',
        'password': 'kidjamo@'
    }

    optimizer = DatabaseIndexOptimizer(db_config)

    # Test optimisation compl√®te
    results = await optimizer.run_complete_optimization()
    print(f"‚ö° Optimisation termin√©e: {len(results['operations'])} op√©rations en {results.get('duration', 0):.1f}s")

    for operation in results['operations']:
        print(f"   - {operation['operation']}: {operation.get('status', 'unknown')}")

if __name__ == "__main__":
    asyncio.run(main())
