# Pipeline IoT Streaming - Kafka Local (RefactorisÃ©e)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com)
[![PySpark](https://img.shields.io/badge/PySpark-3.4+-orange.svg)](https://spark.apache.org)
[![Kafka](https://img.shields.io/badge/Kafka-Local-red.svg)](https://kafka.apache.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-purple.svg)](https://streamlit.io)

## ğŸ—ï¸ Architecture Temps RÃ©el

```mermaid
graph LR
    A[ğŸ“± Bracelets IoT] --> B[ğŸŒ API d'Ingestion]
    B --> C[ğŸ“Š Kafka Topics]
    C --> D[âš¡ PySpark Streaming]
    D --> E[ğŸ’¾ Data Lake]
    C --> F[ğŸ“ˆ Dashboard Temps RÃ©el]
    B --> G[ğŸ—„ï¸ PostgreSQL]
    
    subgraph "Topics Kafka"
        C1[kidjamo-iot-measurements]
        C2[kidjamo-iot-alerts]
        C3[kidjamo-iot-device-status]
        C4[kidjamo-iot-errors]
    end
    
    subgraph "Data Lake Layers"
        E1[RAW - DonnÃ©es brutes]
        E2[BRONZE - AgrÃ©gations]
        E3[SILVER - Alertes critiques]
    end
```

## ğŸš€ Refactoring Complet (No-op Logic)

### âœ… **AmÃ©liorations ApportÃ©es**

**Code Quality & Documentation :**
- ğŸ“– **Docstrings franÃ§aises complÃ¨tes** : RÃ´le, objectifs, entrÃ©es/sorties, effets de bord
- ğŸ·ï¸ **Type hints ajoutÃ©s** : `Optional[float]`, `List[Dict]`, `-> bool` sur fonctions critiques
- ğŸ“ **Commentaires inline dÃ©taillÃ©s** : Seuils mÃ©dicaux, logique mÃ©tier, configurations
- ğŸ”§ **Imports triÃ©s et groupÃ©s** : stdlib, tiers, puis imports locaux
- ğŸ¯ **Constantes isolÃ©es** : Topics Kafka, seuils mÃ©dicaux, configuration

**Architecture & Structure :**
- ğŸ—ï¸ **Classes documentÃ©es** : KafkaManager, IoTStreamingProcessor, RealTimeMonitor
- ğŸ”— **Helpers privÃ©s** : `_percentile()`, `_export_metrics_row()` sans impact comportemental
- ğŸ“Š **Logging structurÃ©** : `logging.getLogger(__name__)`, niveaux appropriÃ©s
- âš¡ **Gestion d'erreurs robuste** : Graceful degradation, fallback gracieux

### ğŸ¯ **Garanties No-op RespectÃ©es**

- âœ… **Endpoints inchangÃ©s** : `/health`, `/iot/measurements`, `/ingest`
- âœ… **Topics Kafka identiques** : Noms exacts et sÃ©rialisation JSON
- âœ… **Seuils mÃ©dicaux conservÃ©s** : SpO2 < 88%, TÂ° â‰¥ 38Â°C, FC 40-180 bpm
- âœ… **Configuration Spark** : Triggers, watermarks, checkpoints intacts
- âœ… **Side-effects identiques** : Kafka, DB, fichiers, logs fonctionnels

## ğŸ“‚ Structure des Composants

### 1. **API d'Ingestion** (`api/iot_ingestion_api.py`)
```
ğŸ¯ RÃ´le : RAW streaming â†’ Kafka
ğŸ“¥ EntrÃ©es : JSON IoT via FastAPI (/iot/measurements, /ingest)
ğŸ“¤ Sorties : Messages Kafka enrichis, mÃ©triques CSV, tables PostgreSQL
ğŸ”§ FonctionnalitÃ©s :
  â€¢ Validation seuils critiques immÃ©diats (SpO2 < 88%, TÂ° â‰¥ 38Â°C)
  â€¢ Publication topics spÃ©cialisÃ©s (measurements, alerts, device_status, errors)
  â€¢ Graceful degradation (mode offline si Kafka indisponible)
  â€¢ Pseudonymisation RGPD (hachage SHA-256 + salt)
```

### 2. **Processeur Streaming** (`streaming/iot_streaming_processor.py`)
```
ğŸ¯ RÃ´le : Kafka â†’ Bronze/Silver/Gold (selon sinks)
ğŸ“¥ EntrÃ©es : Topics Kafka temps rÃ©el + configuration environnement
ğŸ“¤ Sorties : Data Lake Parquet (raw, bronze, silver) + checkpoints
ğŸ”§ FonctionnalitÃ©s :
  â€¢ Auto-sÃ©lection connecteur Kafka selon version PySpark
  â€¢ VÃ©rification environnement Java (compatibilitÃ© 3.x/4.x)
  â€¢ AgrÃ©gations fenÃªtrÃ©es (5 min) avec watermark 10 minutes
  â€¢ DÃ©tection anomalies selon seuils mÃ©dicaux inchangÃ©s
```

### 3. **Dashboard Temps RÃ©el** (`monitoring/realtime_dashboard.py`)
```
ğŸ¯ RÃ´le : Visualisation temps rÃ©el (lecture Kafka)
ğŸ“¥ EntrÃ©es : Topics Kafka (measurements, alerts) + API /health
ğŸ“¤ Sorties : Interface Streamlit avec graphiques Plotly
ğŸ”§ FonctionnalitÃ©s :
  â€¢ Threads daemon pour consommation Kafka en arriÃ¨re-plan
  â€¢ Buffers circulaires (deque) pour historique rÃ©cent
  â€¢ Seuils UI inchangÃ©s (lignes hachurÃ©es mÃ©dicales)
  â€¢ Auto-refresh configurable
```

### 4. **Simulateurs IoT**

#### Simple (`simulator/simple_iot_simulator.py`)
```
ğŸ¯ RÃ´le : DÃ©marrage rapide (3 patients fixes)
ğŸ“¤ Sorties : POST vers /iot/measurements
ğŸ”§ ScÃ©narios : normal (70%), stress (15%), activitÃ© (10%), crise (5%)
ğŸ‘¥ Patients : SS (8 ans), AS (15 ans), SC (12 ans)
```

#### AvancÃ© (`simulator/medical_iot_simulator.py`)
```
ğŸ¯ RÃ´le : Simulation rÃ©aliste multi-patients
ğŸ“¤ Sorties : Kafka + API dual selon disponibilitÃ©
ğŸ”§ FonctionnalitÃ©s :
  â€¢ Cycles circadiens et facteurs environnementaux
  â€¢ GÃ©notypes drÃ©panocytaires (SS/AS/SC) avec paramÃ¨tres spÃ©cifiques
  â€¢ Threading sÃ©curisÃ© (daemon threads par patient)
  â€¢ ScÃ©narios complexes : crises Ã©volutives, infections, dÃ©shydratation
```

### 5. **Tests d'IntÃ©gration** (`tests/test_integration.py`)
```
ğŸ¯ RÃ´le : Validation chaÃ®ne complÃ¨te E2E
ğŸ§ª Tests : API, Kafka, DB, alertes, mÃ©triques, gestion d'erreurs
ğŸ“Š Rapports : Export JSON dans evidence/test_reports/
ğŸ”§ FonctionnalitÃ©s :
  â€¢ Fallback gracieux (mode API-only si Kafka indisponible)
  â€¢ Validation assertions selon seuils critiques
  â€¢ Cleanup automatique des ressources
```

## ğŸš€ Guide de DÃ©marrage

### PrÃ©requis
```bash
# Python 3.8+
python --version

# Java (pour PySpark)
java -version
# PySpark 4.x â†’ Java 17 minimum
# PySpark 3.4.x â†’ Java 8/11 recommandÃ©

# Dependencies
pip install -r requirements.txt
```

### ğŸƒâ€â™‚ï¸ Lancement Rapide

#### 1. **DÃ©marrage de l'API** (Terminal 1)
```bash
cd api
python iot_ingestion_api.py

# Output attendu:
# âœ… Connected to Kafka
# DÃ©marrage du serveur sur http://127.0.0.1:8001
```

#### 2. **Test avec Simulateur Simple** (Terminal 2)
```bash
cd simulator
python simple_iot_simulator.py

# Output attendu:
# âœ… API accessible
# ğŸš€ DÃ©marrage simulation IoT: 3 patients, 30 min
# âœ… patient-001-ss: SpO2=94.2%, HR=115, TÂ°=36.9Â°C - Normal
# ğŸš¨ patient-001-ss: SpO2=85.1%, HR=145, TÂ°=39.2Â°C - 2 alertes!
```

#### 3. **Dashboard en Temps RÃ©el** (Terminal 3)
```bash
cd monitoring
streamlit run realtime_dashboard.py

# Interface web: http://localhost:8501
# Graphiques: TempÃ©rature, FC, SpO2, FrÃ©quence Respiratoire
# Tableau alertes avec codes couleur sÃ©vÃ©ritÃ© (ğŸ”´ğŸŸ ğŸŸ¡ğŸŸ¢)
```

#### 4. **Processeur Streaming** (Terminal 4) - Optionnel
```bash
cd streaming
python iot_streaming_processor.py

# Output attendu:
# ğŸš€ Starting Kidjamo IoT Streaming Pipeline...
# âœ… Started 6 streaming queries
# CrÃ©ation data_lake/raw/iot_measurements/
# CrÃ©ation data_lake/bronze/iot_aggregations/
```

### ğŸ§ª Tests d'IntÃ©gration
```bash
cd tests
python test_integration.py

# Output attendu:
# ğŸ§ª Tests d'IntÃ©gration Pipeline IoT Kidjamo
# âœ… Test API Health Check: PASSED (0.123s)
# âœ… Test API Measurements Normal: PASSED (0.456s)
# âœ… Test Critical Alerts Workflow: PASSED (1.234s)
# ğŸ“ˆ Taux de succÃ¨s: 100.0%
```

## ğŸ“Š Topics Kafka & Data Flow

### Topics SpÃ©cialisÃ©s
```yaml
measurements: kidjamo-iot-measurements  # DonnÃ©es IoT enrichies
alerts:       kidjamo-iot-alerts        # Alertes critiques
device_status: kidjamo-iot-device-status # Statut technique
errors:       kidjamo-iot-errors         # Erreurs de traitement
```

### Structure Messages Kafka
```json
{
  "patient_id": "patient-001-ss",
  "device_id": "uuid-device",
  "timestamp": "2025-08-24T10:30:00.123456",
  "measurements": {
    "freq_card": 120,
    "freq_resp": 18,
    "spo2_pct": 85.0,
    "temp_corp": 38.5
  },
  "ingestion_timestamp": "2025-08-24T10:30:00.789Z",
  "api_version": "1.0.0",
  "message_id": "uuid-unique"
}
```

## ğŸ”¥ Alertes Critiques & Seuils MÃ©dicaux

### RÃ¨gles d'Alertes (InchangÃ©es)
```python
SEUILS_CRITIQUES = {
    "spo2_critical": 88.0,           # SpOâ‚‚ < 88% â†’ CRITICAL
    "temperature_critical": 38.0,    # TÂ° â‰¥ 38Â°C â†’ CRITICAL  
    "heart_rate_min": 40,            # FC < 40 bpm â†’ CRITICAL
    "heart_rate_max": 180,           # FC > 180 bpm â†’ CRITICAL
    "combo_fever_spo2": {            # TrÃ¨s dangereux
        "spo2": 92.0,                # SpOâ‚‚ â‰¤ 92% ET
        "temperature": 38.0           # TÂ° â‰¥ 38Â°C â†’ CRITICAL
    }
}
```

### ScÃ©narios de Simulation
```python
SCENARIOS = {
    "normal": 70%,     # Variations physiologiques normales
    "stress": 15%,     # FC/TÂ° augmentÃ©es, SpOâ‚‚ baissÃ©e
    "activity": 10%,   # FC Ã©levÃ©e, TÂ° augmentÃ©e  
    "crisis": 5%       # SpOâ‚‚ -5 Ã  -15%, TÂ° +1.5 Ã  +3.0Â°C
}
```

## ğŸ“ˆ Data Lake Architecture

```
./data_lake/
â”œâ”€â”€ raw/                    # DonnÃ©es brutes validÃ©es
â”‚   â””â”€â”€ iot_measurements/   # Parquet partitionnÃ© par patient_id
â”œâ”€â”€ bronze/                 # AgrÃ©gations temps rÃ©el
â”‚   â”œâ”€â”€ iot_aggregations/   # FenÃªtres 5min avec anomalies
â”‚   â”œâ”€â”€ iot_alerts/         # Toutes alertes
â”‚   â””â”€â”€ device_status/      # Statut technique
â””â”€â”€ silver/                 # DonnÃ©es critiques
    â””â”€â”€ critical_alerts/    # Alertes nÃ©cessitant action immÃ©diate
```

## ğŸ”§ Configuration & Variables d'Environnement

### API Configuration
```bash
API_HOST=127.0.0.1                    # Host API (dÃ©faut: 127.0.0.1)
API_PORT=8001                         # Port API (dÃ©faut: 8001)
API_PORT_AUTOFALLBACK=true            # Fallback port auto (8002, 8003...)
```

### Kafka Configuration
```bash
KAFKA_STARTING_OFFSETS=latest         # latest|earliest
KAFKA_FAIL_ON_DATA_LOSS=false        # true|false
RESET_STREAMING_CHECKPOINTS=false    # RÃ©initialiser checkpoints
```

### Database Configuration
```bash
PGHOST=localhost                      # PostgreSQL host
PGDATABASE=kidjamo-db                # Database name
PGUSER=postgres                       # Username
PGPASSWORD=kidjamo@                  # Password
PGCONNECT_TIMEOUT=2                  # Timeout connexion
```

## ğŸ“Š MÃ©triques & ObservabilitÃ©

### Endpoint MÃ©triques API
```bash
curl http://localhost:8001/metrics

# RÃ©ponse JSON:
{
  "processed_messages": 1250,
  "kafka_topics": ["kidjamo-iot-measurements", "kidjamo-iot-alerts"],
  "api_uptime_s": 3600.5,
  "latency_p50_s": 0.0125,
  "latency_p95_s": 0.0890,
  "latency_p99_s": 0.1234,
  "throughput_eps": 2.8,
  "valid_pct": 94.5,
  "alerts_by_type": {
    "CRITICAL_SPO2": 15,
    "CRITICAL_TEMPERATURE": 8
  }
}
```

### Export CSV MÃ©triques
```csv
# evidence/metrics/ingestion_metrics.csv
timestamp,latency_s,alerts_count,alert_types,quality_score,valid_flag,device_id
2025-08-24T10:30:00.123Z,0.0156,2,CRITICAL_SPO2|CRITICAL_TEMPERATURE,88.5,1,uuid-device
```

## ğŸ§ª Tests & Validation

### Suite de Tests ComplÃ¨te
```python
TESTS_INTEGRATION = [
    "API Health Check",              # GET /health
    "API Measurements Normal",       # POST /iot/measurements
    "API Measurements Critical",     # Validation alertes gÃ©nÃ©rÃ©es
    "API Ingest Endpoint",          # POST /ingest (tests legacy)
    "Kafka Message Consumption",    # Consommation topics
    "Database Insertion",           # Validation PostgreSQL
    "Critical Alerts Workflow",     # Workflow E2E alertes
    "API Metrics Endpoint",         # GET /metrics
    "Error Handling"                # Gestion erreurs
]
```

### Validation Comportement
- âœ… **No-op strict** : MÃªmes outputs pour mÃªmes inputs
- âœ… **Seuils identiques** : Logique mÃ©dicale inchangÃ©e
- âœ… **Side-effects prÃ©servÃ©s** : Kafka, DB, fichiers
- âœ… **Configuration intacte** : Variables d'environnement

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes Courants

#### 1. **Erreur Java/PySpark**
```bash
# Erreur: UnsupportedClassVersionError
# Solution: Installer JDK compatible
# PySpark 4.x â†’ Java 17
# PySpark 3.4.x â†’ Java 8/11

# VÃ©rification:
java -version
echo $JAVA_HOME
```

#### 2. **Kafka Connection Failed**
```bash
# Erreur: Kafka library unavailable
# Solution: Mode graceful degradation activÃ© automatiquement
# L'API fonctionne sans Kafka (PostgreSQL uniquement)

pip install kafka-python  # Installation manuelle si nÃ©cessaire
```

#### 3. **Port OccupÃ©**
```bash
# Erreur: Port 8001 occupÃ©
# Solution automatique: Fallback 8002, 8003... (si API_PORT_AUTOFALLBACK=true)
# Solution manuelle: API_PORT=8002
```

#### 4. **PostgreSQL Connection**
```bash
# Erreur: DB insert failed (continuing)
# Solution: Mode best-effort, l'API continue sans DB
# Configuration: variables PGHOST, PGUSER, PGPASSWORD
```

## ğŸ“š Documentation AvancÃ©e

### Scripts de DÃ©marrage
- `start_api.bat` : DÃ©marrage API Windows
- `start_pipeline.ps1` : Pipeline complÃ¨te PowerShell
- `start_simple.ps1` : Mode simplifiÃ©
- `test_simulation.ps1` : Tests automatisÃ©s

### Guides DÃ©taillÃ©s
- `QUICK_START.md` : Guide dÃ©marrage rapide
- `GUIDE_SIMULATION_COMPLETE.md` : Simulation avancÃ©e
- `evidence/` : Rapports et mÃ©triques

## ğŸ¯ Roadmap

### Prochaines AmÃ©liorations
- [ ] **Dashboard avancÃ©** : Graphiques historiques, filtres patients
- [ ] **Alertes ML** : DÃ©tection anomalies par machine learning
- [ ] **API Authentication** : JWT, OAuth2 pour production
- [ ] **Kafka Cluster** : Configuration multi-broker
- [ ] **Monitoring AvancÃ©** : Prometheus, Grafana
- [ ] **Tests Performance** : Load testing, stress testing

---

## ğŸ¤ Contribution

### Code Quality Standards
- PEP8 compliance (formatage automatique)
- Type hints obligatoires pour fonctions publiques
- Docstrings franÃ§aises format Google/NumPy
- Tests d'intÃ©gration pour nouveaux endpoints
- No-op logic pour refactoring

### Pull Request Process
1. Fork du repository
2. Feature branch depuis `main`
3. Tests d'intÃ©gration passants
4. Documentation mise Ã  jour
5. Review code + validation comportement

---

**ğŸ“§ Contact :** pipeline-iot@kidjamo.com  
**ğŸ“– Documentation :** [docs/iot-streaming/](../../../docs/)  
**ğŸ› Issues :** [GitHub Issues](https://github.com/kidjamo/pipeline-iot/issues)
