"""
Orchestration des 4 √©tapes dans l'ordre 01=>02=>03=>04.

R√¥le :
    Ex√©cuter s√©quentiellement les scripts de transformation du pipeline
    d'ingestion avec gestion d'erreurs, logs d√©taill√©s et contr√¥le de flux.

Comportement :
    - Ordre d'ex√©cution strict : RAW ‚Üí BRONZE ‚Üí SILVER ‚Üí GOLD
    - V√©rification pr√©requis (Spark, jobs, donn√©es)
    - Ex√©cution avec streaming des logs en temps r√©el
    - Gestion d'erreurs avec option de continuation
    - R√©sum√© final avec m√©triques de performance

Entr√©es :
    - Scripts Python dans data/jobs/ (01_to_raw.py ‚Üí 04_silver_to_gold.py)
    - Donn√©es source dans data/lake/landing/*.csv
    - Environnement Spark configur√©

Sorties :
    - Ex√©cution s√©quentielle des 4 √©tapes
    - Logs d√©taill√©s de progression et erreurs
    - Data Lake complet : raw ‚Üí bronze ‚Üí silver ‚Üí gold
    - Rapport final de performance

Effets de bord :
    - Configuration environnement PYTHONPATH pour imports
    - Nettoyage variables Hadoop/Spark invalides
    - Cr√©ation arborescence Data Lake compl√®te
    - Interaction utilisateur pour continuation sur erreur
"""

import os
import subprocess
import sys
import time
from datetime import datetime
from typing import List, Optional, Tuple

# Configuration des r√©pertoires bas√©e sur la localisation du script
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
JOBS_DIR = os.path.join(BASE_DIR, "jobs")
LAKE_DIR = os.path.join(BASE_DIR, "lake")
PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, ".."))
INGESTION_DIR = os.path.join(PROJECT_ROOT, "ingestion")

# Configuration des chemins Spark possibles (Windows)
SPARK_CANDIDATES = [
    "spark-submit",  # Si dans le PATH
    "C:\\spark-3.5.5-bin-hadoop3\\spark-3.5.5-bin-hadoop3\\bin\\spark-submit.cmd",
    "C:\\spark-3.5.5-bin-hadoop3\\bin\\spark-submit.cmd",
    "C:\\spark\\bin\\spark-submit.cmd"
]

# Configuration du pipeline
PIPELINE_JOBS = [
    {
        "script": "01_to_raw.py",
        "name": "01 - Landing to Raw",
        "description": "Ingestion des donn√©es depuis landing vers raw"
    },
    {
        "script": "02_raw_to_bronze.py",
        "name": "02 - Raw to Bronze",
        "description": "Transformation et nettoyage vers bronze"
    },
    {
        "script": "03_bronze_to_silver.py",
        "name": "03 - Bronze to Silver",
        "description": "Enrichissement et validation vers silver"
    },
    {
        "script": "04_silver_to_gold.py",
        "name": "04 - Silver to Gold",
        "description": "Agr√©gations et features ML vers gold"
    }
]


def log_message(message: str, level: str = "INFO") -> None:
    """
    Affiche un message avec horodatage standardis√©.

    Args:
        message: Message √† afficher
        level: Niveau de log (INFO, ERROR, WARNING)
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {level}: {message}")


def _find_spark_installation() -> Optional[str]:
    """
    Recherche une installation Spark valide dans les chemins candidats.

    Returns:
        str: Chemin vers spark-submit si trouv√©, None sinon
    """
    for spark_path in SPARK_CANDIDATES:
        try:
            subprocess.run([spark_path, "--version"],
                          capture_output=True, check=True, timeout=30)
            return spark_path
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
            continue
    return None


def _validate_job_files() -> bool:
    """
    V√©rifie l'existence de tous les fichiers de jobs requis.

    Returns:
        bool: True si tous les jobs existent, False sinon
    """
    for job_config in PIPELINE_JOBS:
        job_path = os.path.join(JOBS_DIR, job_config["script"])
        if not os.path.exists(job_path):
            log_message(f"‚ùå Job manquant: {job_path}", "ERROR")
            return False
    return True


def check_prerequisites() -> Tuple[bool, Optional[str]]:
    """
    V√©rifie que tous les pr√©requis sont pr√©sents pour l'ex√©cution.

    V√©rifications effectu√©es :
    - Installation Spark accessible
    - R√©pertoire des jobs existant
    - Tous les fichiers de jobs pr√©sents

    Returns:
        tuple: (success: bool, spark_command: Optional[str])
    """
    log_message("V√©rification des pr√©requis...")

    # 1. V√©rification installation Spark
    spark_command = _find_spark_installation()
    if spark_command is None:
        log_message("‚ùå Spark n'est pas disponible. V√©rifiez votre installation.", "ERROR")
        log_message("Chemins test√©s:", "ERROR")
        for path in SPARK_CANDIDATES:
            log_message(f"  - {path}", "ERROR")
        return False, None

    log_message(f"‚úÖ Spark trouv√© √†: {spark_command}")

    # 2. V√©rification r√©pertoire des jobs
    if not os.path.exists(JOBS_DIR):
        log_message(f"‚ùå R√©pertoire des jobs introuvable: {JOBS_DIR}", "ERROR")
        log_message("Veuillez ex√©cuter ce script depuis data\\pipelines", "ERROR")
        return False, None

    # 3. V√©rification fichiers de jobs
    if not _validate_job_files():
        return False, None

    log_message("‚úÖ Tous les pr√©requis sont satisfaits")
    return True, spark_command


def check_data_availability() -> bool:
    """
    V√©rifie la disponibilit√© des donn√©es sources dans landing.

    Returns:
        bool: True si donn√©es disponibles, False sinon
    """
    log_message("V√©rification de la disponibilit√© des donn√©es...")

    landing_dir = os.path.join(LAKE_DIR, "landing")
    if os.path.exists(landing_dir) and os.listdir(landing_dir):
        files = os.listdir(landing_dir)
        log_message(f"‚úÖ Donn√©es trouv√©es dans {landing_dir}: {len(files)} fichier(s)")

        # Affichage des premiers fichiers pour information
        for file in files[:5]:
            log_message(f"    - {file}")
        return True
    else:
        log_message(f"‚ö†Ô∏è  Aucune donn√©e trouv√©e dans {landing_dir}", "WARNING")
        return False


def _setup_execution_environment() -> dict:
    """
    Configure l'environnement d'ex√©cution pour les sous-processus.

    Configuration :
    - Ajout ingestion et data au PYTHONPATH
    - Propagation SPARK_HOME si d√©tect√©
    - Nettoyage HADOOP_HOME invalide

    Returns:
        dict: Environnement configur√© pour subprocess
    """
    env = os.environ.copy()

    # Configuration PYTHONPATH pour imports cross-modules
    pythonpath = env.get("PYTHONPATH", "")
    add_paths = [INGESTION_DIR, BASE_DIR]
    for path in add_paths:
        if path and path not in pythonpath:
            pythonpath = (pythonpath + (os.pathsep if pythonpath else "") + path)
    env["PYTHONPATH"] = pythonpath

    return env


def _configure_spark_environment(env: dict, spark_command: Optional[str]) -> None:
    """
    Configure l'environnement Spark dans le dictionnaire d'environnement.

    Args:
        env: Dictionnaire d'environnement √† modifier
        spark_command: Chemin vers spark-submit si disponible
    """
    try:
        if (spark_command and os.path.isabs(spark_command) and
            os.path.basename(spark_command).startswith("spark-submit")):

            bin_dir = os.path.dirname(spark_command)
            spark_home = os.path.abspath(os.path.join(bin_dir, ".."))
            env["SPARK_HOME"] = spark_home

            spark_bin = os.path.join(spark_home, "bin")
            current_path = env.get("PATH", "")
            if spark_bin and spark_bin not in current_path:
                env["PATH"] = spark_bin + os.pathsep + current_path
    except Exception:
        pass


def _cleanup_hadoop_environment(env: dict) -> None:
    """
    Nettoie HADOOP_HOME invalide pour √©viter erreurs winutils sur Windows.

    Args:
        env: Dictionnaire d'environnement √† nettoyer
    """
    try:
        hadoop_home = env.get("HADOOP_HOME")
        if hadoop_home and not os.path.isdir(hadoop_home):
            env.pop("HADOOP_HOME", None)
    except Exception:
        pass


def run_spark_job(job_path: str, job_name: str, spark_command: Optional[str]) -> bool:
    """
    Ex√©cute un job Spark avec streaming des logs en temps r√©el.

    Configuration d'ex√©cution :
    - Ex√©cution directe avec Python (plus fiable que spark-submit)
    - Environnement configur√© avec PYTHONPATH et variables Spark
    - Streaming des logs en temps r√©el avec pr√©fixe d'indentation
    - Gestion timeout et codes de retour

    Args:
        job_path: Chemin vers le script Python √† ex√©cuter
        job_name: Nom du job pour logging
        spark_command: Chemin spark-submit pour configuration environnement

    Returns:
        bool: True si succ√®s, False si √©chec
    """
    log_message(f"D√©marrage du job: {job_name}")
    start_time = time.time()

    try:
        # Configuration environnement d'ex√©cution
        env = _setup_execution_environment()
        _configure_spark_environment(env, spark_command)
        _cleanup_hadoop_environment(env)

        # Lancement du processus avec streaming des logs
        proc = subprocess.Popen(
            ["python", "-u", job_path],
            cwd=BASE_DIR,
            env=env,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
        )

        # Streaming output en temps r√©el avec indentation
        if proc.stdout is not None:
            for line in iter(proc.stdout.readline, ''):
                if not line:
                    break
                line = line.rstrip('\n')
                if line:
                    print(f"    {line}")

        return_code = proc.wait()
        end_time = time.time()
        duration = end_time - start_time

        # √âvaluation r√©sultat et logging
        if return_code == 0:
            log_message(f"‚úÖ {job_name} termin√© avec succ√®s en {duration:.1f}s")
            return True
        else:
            log_message(f"‚ùå {job_name} a √©chou√© apr√®s {duration:.1f}s", "ERROR")
            log_message("Voir les logs ci-dessus pour le d√©tail des erreurs.", "ERROR")
            return False

    except Exception as e:
        log_message(f"‚ùå Erreur inattendue pour {job_name}: {str(e)}", "ERROR")
        return False


def _ask_user_continuation(step_number: int) -> bool:
    """
    Demande √† l'utilisateur s'il souhaite continuer apr√®s une erreur.

    Args:
        step_number: Num√©ro de l'√©tape qui a √©chou√©

    Returns:
        bool: True si l'utilisateur veut continuer, False sinon
    """
    try:
        response = input(f"\n√âtape {step_number} a √©chou√©. Continuer avec l'√©tape suivante ? (y/N): ")
        return response.lower() == 'y'
    except (EOFError, KeyboardInterrupt):
        return False


def _print_pipeline_summary(successful_jobs: int, failed_jobs: int, total_duration: float) -> None:
    """
    Affiche le r√©sum√© final du pipeline avec m√©triques.

    Args:
        successful_jobs: Nombre de jobs r√©ussis
        failed_jobs: Nombre de jobs √©chou√©s
        total_duration: Dur√©e totale d'ex√©cution en secondes
    """
    log_message("=" * 60)
    log_message("üèÅ R√âSUM√â DU PIPELINE")
    log_message(f"Dur√©e totale: {total_duration:.1f} secondes")
    log_message(f"Jobs r√©ussis: {successful_jobs}")
    log_message(f"Jobs √©chou√©s: {failed_jobs}")

    if failed_jobs == 0:
        log_message("PIPELINE TERMIN√â AVEC SUCC√àS !")
        log_message("Donn√©es disponibles dans le Data Lake:")
        log_message(f"  - {os.path.join(LAKE_DIR, 'raw')}     : Donn√©es brutes")
        log_message(f"  - {os.path.join(LAKE_DIR, 'bronze')}  : Donn√©es nettoy√©es")
        log_message(f"  - {os.path.join(LAKE_DIR, 'silver')}  : Donn√©es enrichies")
        log_message(f"  - {os.path.join(LAKE_DIR, 'gold')}    : Agr√©gations et features ML")
    else:
        log_message(f"‚ö†Ô∏è  Pipeline termin√© avec {failed_jobs} erreur(s)", "WARNING")
        log_message("V√©rifiez les logs ci-dessus pour plus de d√©tails")

    log_message("=" * 60)


def main() -> None:
    """
    Fonction principale d'orchestration du pipeline complet.

    Orchestration compl√®te :
    1. V√©rifications pr√©requis (Spark, jobs, donn√©es)
    2. Ex√©cution s√©quentielle des 4 √©tapes du pipeline
    3. Gestion d'erreurs avec option de continuation
    4. Pauses entre √©tapes pour stabilit√©
    5. R√©sum√© final avec m√©triques de performance

    Gestion d'erreurs :
    - Arr√™t sur pr√©requis manquants
    - Option continuation sur √©chec d'√©tape
    - Interruption propre sur Ctrl+C
    """
    log_message("üöÄ D√âMARRAGE DU PIPELINE D'INGESTION COMPLET")
    log_message("=" * 60)

    # 1. V√©rifications pr√©liminaires
    prereq_ok, spark_command = check_prerequisites()
    if not prereq_ok:
        log_message("‚ùå Les pr√©requis ne sont pas satisfaits. Arr√™t du pipeline.", "ERROR")
        sys.exit(1)

    check_data_availability()

    # 2. Pr√©paration configuration des jobs avec chemins complets
    jobs_with_paths = [
        {
            **job_config,
            "path": os.path.join(JOBS_DIR, job_config["script"])
        }
        for job_config in PIPELINE_JOBS
    ]

    # 3. Ex√©cution s√©quentielle du pipeline
    pipeline_start_time = time.time()
    successful_jobs = 0
    failed_jobs = 0

    for i, job in enumerate(jobs_with_paths, 1):
        # Affichage en-t√™te √©tape
        log_message("=" * 60)
        log_message(f"√âTAPE {i}/4: {job['name']}")
        log_message(f"Description: {job['description']}")
        log_message("-" * 40)

        # Ex√©cution job avec mesure de performance
        success = run_spark_job(job["path"], job["name"], spark_command)

        if success:
            successful_jobs += 1
            log_message(f"‚úÖ √©tape {i} termin√©e avec succ√®s")
        else:
            failed_jobs += 1
            log_message(f"‚ùå √âtape {i} a √©chou√©", "ERROR")

            # Gestion continuation sur erreur
            if not _ask_user_continuation(i):
                log_message("Pipeline interrompu par l'utilisateur", "WARNING")
                break

        # Pause inter-√©tapes pour stabilit√© syst√®me
        if i < len(jobs_with_paths):
            log_message("Pause de 2 secondes avant l'√©tape suivante...")
            time.sleep(2)

    # 4. R√©sum√© final avec m√©triques
    pipeline_end_time = time.time()
    total_duration = pipeline_end_time - pipeline_start_time
    _print_pipeline_summary(successful_jobs, failed_jobs, total_duration)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log_message("\nüõë Pipeline interrompu par l'utilisateur", "WARNING")
        sys.exit(1)
    except Exception as e:
        log_message(f"‚ùå Erreur inattendue: {str(e)}", "ERROR")
        sys.exit(1)
