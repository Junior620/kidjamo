"""
Amazon Bedrock Engine - Alternative professionnelle √† Gemini Flash
Support multi-mod√®les avec Claude, Llama, et Titan
"""

import boto3
import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import time
from botocore.exceptions import ClientError, BotoCoreError

logger = logging.getLogger(__name__)

class BedrockAIEngine:
    """Moteur IA Amazon Bedrock pour production m√©dicale"""

    def __init__(self):
        # Configuration AWS
        self.aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
        self.aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
        self.aws_region = os.getenv('AWS_REGION', 'us-east-1')

        if not self.aws_access_key or not self.aws_secret_key:
            raise ValueError("Cl√©s AWS non trouv√©es dans les variables d'environnement")

        # Initialisation client Bedrock
        try:
            self.bedrock_client = boto3.client(
                'bedrock-runtime',
                aws_access_key_id=self.aws_access_key,
                aws_secret_access_key=self.aws_secret_key,
                region_name=self.aws_region
            )
            logger.info(f"Client Bedrock initialis√© dans la r√©gion {self.aws_region}")
        except Exception as e:
            logger.error(f"Erreur initialisation Bedrock: {e}")
            raise

        # Configuration des mod√®les disponibles
        self.available_models = {
            "claude-3-haiku": {
                "id": "anthropic.claude-3-haiku-20240307-v1:0",
                "max_tokens": 4096,
                "cost_per_1k_input": 0.00025,  # $0.25/1M tokens
                "cost_per_1k_output": 0.00125,  # $1.25/1M tokens
                "recommended": True,
                "use_case": "Rapide et √©conomique"
            },
            "claude-3-sonnet": {
                "id": "anthropic.claude-3-sonnet-20240229-v1:0",
                "max_tokens": 4096,
                "cost_per_1k_input": 0.003,    # $3/1M tokens
                "cost_per_1k_output": 0.015,   # $15/1M tokens
                "recommended": False,
                "use_case": "Qualit√© sup√©rieure"
            },
            "llama-3-8b": {
                "id": "meta.llama3-8b-instruct-v1:0",
                "max_tokens": 2048,
                "cost_per_1k_input": 0.0003,   # $0.3/1M tokens
                "cost_per_1k_output": 0.0006,  # $0.6/1M tokens
                "recommended": True,
                "use_case": "Open source, √©conomique"
            },
            "titan-text": {
                "id": "amazon.titan-text-express-v1",
                "max_tokens": 8192,
                "cost_per_1k_input": 0.0008,   # $0.8/1M tokens
                "cost_per_1k_output": 0.0016,  # $1.6/1M tokens
                "recommended": False,
                "use_case": "Amazon natif"
            }
        }

        # Mod√®le par d√©faut (recommand√© pour m√©dical)
        self.primary_model = "claude-3-haiku"
        self.fallback_model = "llama-3-8b"

        # Configuration production
        self.request_timeout = 30
        self.max_retries = 3
        self.conversation_contexts = {}

        # Cache et m√©triques
        self.response_cache = {}
        self.cache_ttl = 1800  # 30 minutes
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'cost_estimate': 0.0,
            'models_used': {}
        }

        logger.info(f"BedrockAIEngine initialis√© avec mod√®le principal: {self.primary_model}")

    def process_message_with_ai(self, user_message: str, context: Dict) -> Dict:
        """Traite le message avec Amazon Bedrock"""

        session_id = context.get('session_id', 'default')
        self.metrics['total_requests'] += 1

        try:
            # NOUVEAU: V√©rifier d'abord si c'est une question simple (heure, salutation, etc.)
            simple_response = self._handle_simple_questions(user_message, session_id)
            if simple_response:
                logger.info(f"Question simple d√©tect√©e pour session {session_id}")
                return simple_response

            # V√©rification du cache
            cache_key = self._get_cache_key(user_message, session_id)
            cached_response = self._get_from_cache(cache_key)
            if cached_response:
                logger.info(f"R√©ponse servie depuis le cache pour session {session_id}")
                return cached_response

            # S√©lection du mod√®le selon l'urgence
            model_name = self._select_model_for_context(user_message, context)
            model_config = self.available_models[model_name]

            # Construire le prompt m√©dical sp√©cialis√©
            system_prompt = self._build_medical_prompt(user_message, session_id, context)

            # G√©n√©rer la r√©ponse avec le mod√®le s√©lectionn√©
            response = self._generate_with_bedrock(
                model_name,
                model_config["id"],
                system_prompt,
                user_message
            )

            if response:
                # Construire la r√©ponse finale
                final_response = {
                    "response": self._format_response_as_html(response["content"], user_message),
                    "conversation_type": self._detect_conversation_type(user_message),
                    "source": "amazon-bedrock",
                    "model_used": model_name,
                    "model_id": model_config["id"],
                    "raw_response": response["content"],
                    "success": True,
                    "cached": False,
                    "cost_estimate": response["cost"],
                    "tokens_used": response.get("tokens", {})
                }

                # Stocker en cache
                self._store_in_cache(cache_key, final_response)

                # Sauvegarder le contexte
                self._save_conversation_context(session_id, user_message, response["content"])

                # Mettre √† jour les m√©triques
                self.metrics['successful_requests'] += 1
                self.metrics['cost_estimate'] += response["cost"]
                self.metrics['models_used'][model_name] = self.metrics['models_used'].get(model_name, 0) + 1

                logger.info(f"R√©ponse Bedrock g√©n√©r√©e avec {model_name} pour session {session_id}")
                return final_response
            else:
                return self._fallback_response(user_message, "bedrock_error")

        except Exception as e:
            logger.error(f"Erreur critique Bedrock: {e}")
            self.metrics['failed_requests'] += 1
            return self._fallback_response(user_message, "system_error")

    def _select_model_for_context(self, user_message: str, context: Dict) -> str:
        """S√©lectionne le mod√®le optimal selon le contexte"""

        urgency_level = context.get('urgency_level', 'normal')
        message_lower = user_message.lower()

        # Pour les urgences m√©dicales critiques -> Claude Haiku (rapide et fiable)
        if urgency_level == 'critical' or any(word in message_lower for word in ["poitrine", "respir", "8/10", "9/10", "10/10"]):
            return "claude-3-haiku"

        # Pour les questions complexes -> Claude Haiku (meilleure compr√©hension)
        elif len(user_message) > 200 or "comment" in message_lower:
            return "claude-3-haiku"

        # Pour les questions simples -> Llama (√©conomique)
        else:
            return "llama-3-8b"

    def _generate_with_bedrock(self, model_name: str, model_id: str, system_prompt: str, user_message: str) -> Optional[Dict]:
        """G√©n√®re une r√©ponse avec un mod√®le Bedrock sp√©cifique"""

        try:
            model_config = self.available_models[model_name]

            # Construire le payload selon le type de mod√®le
            if "claude" in model_id:
                payload = self._build_claude_payload(system_prompt, user_message, model_config)
            elif "llama" in model_id:
                payload = self._build_llama_payload(system_prompt, user_message, model_config)
            elif "titan" in model_id:
                payload = self._build_titan_payload(system_prompt, user_message, model_config)
            else:
                raise ValueError(f"Type de mod√®le non support√©: {model_id}")

            # Appel API Bedrock avec retry
            response = self._make_bedrock_call_with_retry(model_id, payload)

            if response:
                # Parser la r√©ponse selon le type de mod√®le
                parsed_response = self._parse_model_response(model_id, response)

                # Calculer le co√ªt estim√©
                cost = self._calculate_cost(model_config, parsed_response.get("tokens", {}))

                return {
                    "content": parsed_response["content"],
                    "cost": cost,
                    "tokens": parsed_response.get("tokens", {})
                }

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration {model_name}: {e}")
            return None

    def _build_claude_payload(self, system_prompt: str, user_message: str, config: Dict) -> Dict:
        """Construit le payload pour Claude"""
        return {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": min(config["max_tokens"], 800),
            "temperature": 0.3,
            "top_p": 0.8,
            "system": system_prompt,
            "messages": [
                {
                    "role": "user",
                    "content": user_message
                }
            ]
        }

    def _build_llama_payload(self, system_prompt: str, user_message: str, config: Dict) -> Dict:
        """Construit le payload pour Llama"""
        full_prompt = f"{system_prompt}\n\nHuman: {user_message}\n\nAssistant:"

        return {
            "prompt": full_prompt,
            "max_gen_len": min(config["max_tokens"], 600),
            "temperature": 0.3,
            "top_p": 0.8
        }

    def _build_titan_payload(self, system_prompt: str, user_message: str, config: Dict) -> Dict:
        """Construit le payload pour Titan"""
        full_prompt = f"{system_prompt}\n\nQuestion: {user_message}\nR√©ponse:"

        return {
            "inputText": full_prompt,
            "textGenerationConfig": {
                "maxTokenCount": min(config["max_tokens"], 700),
                "temperature": 0.3,
                "topP": 0.8
            }
        }

    def _make_bedrock_call_with_retry(self, model_id: str, payload: Dict) -> Optional[Dict]:
        """Effectue l'appel Bedrock avec retry automatique"""

        for attempt in range(self.max_retries):
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=model_id,
                    contentType="application/json",
                    accept="application/json",
                    body=json.dumps(payload)
                )

                response_body = json.loads(response['body'].read())
                return response_body

            except ClientError as e:
                error_code = e.response['Error']['Code']
                if error_code == 'ThrottlingException':
                    wait_time = (2 ** attempt) + 1
                    logger.warning(f"Throttling Bedrock - Attente {wait_time}s (tentative {attempt + 1})")
                    time.sleep(wait_time)
                elif error_code == 'ValidationException':
                    logger.error(f"Erreur validation Bedrock: {e}")
                    break
                else:
                    logger.error(f"Erreur Bedrock {error_code}: {e}")
                    if attempt < self.max_retries - 1:
                        time.sleep(2 ** attempt)

            except Exception as e:
                logger.error(f"Erreur r√©seau Bedrock - Tentative {attempt + 1}: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)

        return None

    def _parse_model_response(self, model_id: str, response: Dict) -> Dict:
        """Parse la r√©ponse selon le type de mod√®le"""

        try:
            if "claude" in model_id:
                content = response["content"][0]["text"]
                tokens = {
                    "input": response.get("usage", {}).get("input_tokens", 0),
                    "output": response.get("usage", {}).get("output_tokens", 0)
                }
            elif "llama" in model_id:
                content = response["generation"]
                tokens = {
                    "input": response.get("prompt_token_count", 0),
                    "output": response.get("generation_token_count", 0)
                }
            elif "titan" in model_id:
                content = response["results"][0]["outputText"]
                tokens = {
                    "input": response.get("inputTextTokenCount", 0),
                    "output": response.get("results", [{}])[0].get("tokenCount", 0)
                }
            else:
                raise ValueError(f"Parser non disponible pour {model_id}")

            return {
                "content": content.strip(),
                "tokens": tokens
            }

        except (KeyError, IndexError, TypeError) as e:
            logger.error(f"Erreur parsing r√©ponse {model_id}: {e}")
            return {"content": "Erreur de traitement de la r√©ponse", "tokens": {}}

    def _calculate_cost(self, model_config: Dict, tokens: Dict) -> float:
        """Calcule le co√ªt estim√© de la requ√™te"""
        try:
            input_tokens = tokens.get("input", 0)
            output_tokens = tokens.get("output", 0)

            input_cost = (input_tokens / 1000) * model_config["cost_per_1k_input"]
            output_cost = (output_tokens / 1000) * model_config["cost_per_1k_output"]

            return round(input_cost + output_cost, 6)
        except:
            return 0.0

    def _build_medical_prompt(self, user_message: str, session_id: str, context: Dict) -> str:
        """Construit un prompt m√©dical contextualis√© pour Bedrock"""

        # Informations patient si disponibles
        patient_info = context.get('patient_info', {})
        age = patient_info.get('age', 'Non sp√©cifi√©')

        base_prompt = f"""Tu es Kidjamo Assistant, un assistant m√©dical AI sp√©cialis√© dans l'accompagnement des patients atteints de dr√©panocytose au Cameroun.

CONTEXTE PATIENT:
- √Çge: {age}
- Session: {session_id}
- Niveau d'urgence: {context.get('urgency_level', 'normal')}

PROTOCOLE M√âDICAL STRICT:
- Tu ne remplaces JAMAIS un m√©decin qualifi√©
- URGENCE VITALE si: douleur >7/10, difficult√©s respiratoires, fi√®vre >38.5¬∞C
- En urgence: directive IMM√âDIATE vers services d'urgence camerounais
- Domaine STRICT: dr√©panocytose uniquement
- R√©ponds en fran√ßais simple et accessible

SERVICES D'URGENCE CAMEROUN:
- 1510 (Urgence nationale Cameroun)
- H√¥pital Central Yaound√© - Urgences
- H√¥pital G√©n√©ral Douala - Service d'urgence  
- CHU Yaound√© - Centre r√©f√©rence dr√©panocytose

CENTRES SP√âCIALIS√âS:
- CHU Yaound√© - H√©matologie sp√©cialis√©e
- H√¥pital Laquintinie Douala - Suivi dr√©panocytose
- Centre Pasteur Cameroun - Expertise dr√©panocytose

M√âDICAMENTS DR√âPANOCYTOSE:
- Hydroxyur√©e (Siklos) - Traitement de fond
- Antalgiques (parac√©tamol, anti-inflammatoires)
- Acide folique - Suppl√©ment vital
- Antibiotiques pr√©ventifs

INSTRUCTIONS DE R√âPONSE:
- Sois empathique et rassurant
- Structure avec des √©mojis (üö® ü©∫ üíä)
- Si urgence: priorise ABSOLUMENT la s√©curit√©
- Donne des conseils pratiques et pr√©cis
- Termine par une question de suivi si appropri√©"""

        # Ajouter contexte conversation r√©cente
        if session_id in self.conversation_contexts:
            recent = self.conversation_contexts[session_id][-2:]  # 2 derniers √©changes
            if recent:
                base_prompt += "\n\nCONTEXTE CONVERSATION R√âCENTE:\n"
                for ctx in recent:
                    base_prompt += f"Patient: {ctx['user']}\nAssistant: {ctx['bot'][:100]}...\n"

        return base_prompt

    def _detect_conversation_type(self, message: str) -> str:
        """D√©tecte le type de conversation avec analyse avanc√©e"""
        message_lower = message.lower()

        # Mots-cl√©s d'urgence critique
        critical_keywords = ["mal", "poitrine", "respir", "aide", "urgent", "grave", "intense", "8/10", "9/10", "10/10"]
        pain_keywords = ["douleur", "mal", "/10", "souffre", "crise", "insupportable"]
        medication_keywords = ["m√©dicament", "siklos", "traitement", "pilule", "dose", "oubli"]

        if any(keyword in message_lower for keyword in critical_keywords):
            return "emergency"
        elif any(keyword in message_lower for keyword in pain_keywords):
            return "pain_management"
        elif any(keyword in message_lower for keyword in medication_keywords):
            return "medication"
        else:
            return "general"

    def _format_response_as_html(self, ai_response: str, user_message: str) -> str:
        """Formate la r√©ponse IA en HTML s√©curis√©"""

        # D√©tection d'urgence
        is_emergency = any(word in user_message.lower() for word in ["mal", "poitrine", "respir", "aide", "urgent"])

        if is_emergency:
            css_class = "response-section emergency-alert"
            icon = "fas fa-exclamation-triangle"
        else:
            css_class = "response-section medical-info"
            icon = "fas fa-user-md"

        # Nettoyage s√©curis√© du contenu
        import html
        safe_response = html.escape(ai_response)

        # Conversion markdown basique
        safe_response = safe_response.replace('\n\n', '</p><p>')
        safe_response = safe_response.replace('**', '<strong>').replace('**', '</strong>')

        html_response = f"""
        <div class="{css_class}">
            <h3><i class="{icon}"></i> Kidjamo Assistant</h3>
            <p>{safe_response}</p>
            <div class="bedrock-footer">
                <small>R√©ponse g√©n√©r√©e par Amazon Bedrock - Ne remplace pas un avis m√©dical</small>
            </div>
        </div>
        """

        return html_response

    def _get_cache_key(self, user_message: str, session_id: str) -> str:
        """G√©n√®re une cl√© de cache"""
        return f"{session_id}:{hash(user_message.lower().strip())}"

    def _get_from_cache(self, cache_key: str) -> Optional[Dict]:
        """R√©cup√®re une r√©ponse du cache si disponible"""
        if cache_key in self.response_cache:
            cached_data = self.response_cache[cache_key]
            if datetime.now().timestamp() - cached_data['timestamp'] < self.cache_ttl:
                self.metrics['cache_hits'] += 1
                return cached_data['response']
        return None

    def _store_in_cache(self, cache_key: str, response: Dict):
        """Stocke une r√©ponse dans le cache"""
        self.response_cache[cache_key] = {
            'response': response,
            'timestamp': datetime.now().timestamp()
        }

    def _save_conversation_context(self, session_id: str, user_message: str, ai_response: str):
        """Sauvegarde le contexte de conversation"""
        if session_id not in self.conversation_contexts:
            self.conversation_contexts[session_id] = []

        self.conversation_contexts[session_id].append({
            "user": user_message,
            "bot": ai_response,
            "timestamp": datetime.now().isoformat()
        })

        # Limiter √† 10 √©changes par session
        if len(self.conversation_contexts[session_id]) > 10:
            self.conversation_contexts[session_id] = self.conversation_contexts[session_id][-10:]

    def _fallback_response(self, user_message: str, error_type: str) -> Dict:
        """R√©ponses de secours si Bedrock √©choue"""

        message_lower = user_message.lower()

        if any(word in message_lower for word in ["mal", "poitrine", "respir", "aide", "urgent"]):
            html_response = """
            <div class="response-section emergency-alert">
                <h3><i class="fas fa-ambulance"></i> PROTOCOLE D'URGENCE ACTIV√â</h3>
                <p><strong>Douleur thoracique ou difficult√©s respiratoires d√©tect√©es</strong></p>
                <ul class="urgent-list">
                    <li><strong>APPELEZ IMM√âDIATEMENT:</strong></li>
                    <li><span class="emergency-number">1510</span> Urgence nationale Cameroun</li>
                    <li><strong>CHU Yaound√©</strong> - Centre sp√©cialis√© dr√©panocytose</li>
                    <li><strong>H√¥pital Central Yaound√©</strong> - Service urgences</li>
                </ul>
                <p>‚ö†Ô∏è <strong>Mentionnez "patient dr√©panocytaire"</strong></p>
                <p>Actions imm√©diates: restez calme, position confortable, documents m√©dicaux pr√™ts</p>
            </div>
            """
        else:
            html_response = """
            <div class="response-section medical-info">
                <h3><i class="fas fa-user-md"></i> Assistant Kidjamo</h3>
                <p>Service m√©dical sp√©cialis√© dr√©panocytose Cameroun:</p>
                <ul class="help-list">
                    <li>ü©∫ Gestion crises douloureuses</li>
                    <li>üíä Suivi m√©dicamenteux (Siklos, antalgiques)</li>
                    <li>üö® Protocoles urgence dr√©panocytose</li>
                    <li>üìö √âducation th√©rapeutique</li>
                </ul>
                <p>Reformulez votre question pour une assistance personnalis√©e</p>
            </div>
            """

        return {
            "response": html_response,
            "conversation_type": "fallback",
            "source": f"bedrock-fallback-{error_type}",
            "success": True,
            "cached": False
        }

    def get_bedrock_metrics(self) -> Dict[str, Any]:
        """M√©triques d√©taill√©es pour monitoring Bedrock"""

        # Calcul des co√ªts par mod√®le
        cost_breakdown = {}
        for model_name, usage_count in self.metrics['models_used'].items():
            model_config = self.available_models[model_name]
            estimated_cost = usage_count * 0.002  # Estimation moyenne par requ√™te
            cost_breakdown[model_name] = {
                "requests": usage_count,
                "estimated_cost": estimated_cost,
                "cost_per_request": estimated_cost / max(usage_count, 1)
            }

        return {
            "status": "operational",
            "service": "amazon-bedrock",
            "region": self.aws_region,
            "primary_model": self.primary_model,
            "fallback_model": self.fallback_model,
            "performance": {
                "total_requests": self.metrics['total_requests'],
                "successful_requests": self.metrics['successful_requests'],
                "failed_requests": self.metrics['failed_requests'],
                "success_rate": (self.metrics['successful_requests'] / max(self.metrics['total_requests'], 1)) * 100,
                "cache_hit_rate": (self.metrics['cache_hits'] / max(self.metrics['total_requests'], 1)) * 100
            },
            "costs": {
                "total_estimated": self.metrics['cost_estimate'],
                "average_per_request": self.metrics['cost_estimate'] / max(self.metrics['total_requests'], 1),
                "breakdown_by_model": cost_breakdown
            },
            "models_usage": self.metrics['models_used'],
            "cache_size": len(self.response_cache),
            "active_sessions": len(self.conversation_contexts)
        }

    def get_available_models(self) -> Dict[str, Any]:
        """Retourne la liste des mod√®les disponibles avec leurs caract√©ristiques"""
        return {
            "models": self.available_models,
            "current_primary": self.primary_model,
            "current_fallback": self.fallback_model,
            "selection_strategy": "Automatique selon urgence et complexit√©"
        }

    def _handle_simple_questions(self, user_message: str, session_id: str) -> Optional[Dict]:
        """G√®re les questions simples comme l'heure, la date, ou les salutations"""

        message_lower = user_message.lower().strip()

        # D√©tection sp√©cifique des demandes d'heure
        time_patterns = [
            "il est quelle heure",
            "quelle heure il est",
            "quelle heure",
            "l'heure",
            "heure actuelle",
            "heure qu'il est",
            "temps actuel",
            "horaire"
        ]

        # V√©rifier si c'est une demande d'heure
        if any(pattern in message_lower for pattern in time_patterns):
            current_time = datetime.now().strftime("%H:%M")
            current_date = datetime.now().strftime("%d/%m/%Y")

            html_response = f"""
            <div class="response-section info-section">
                <h3><i class="fas fa-clock"></i> Heure actuelle</h3>
                <p><strong>Il est actuellement {current_time}</strong></p>
                <p>Date: {current_date}</p>
                <p>Si vous avez besoin d'aide m√©dicale concernant votre traitement contre la dr√©panocytose, n'h√©sitez pas √† me poser vos questions!</p>
            </div>
            """

            return {
                "response": html_response,
                "conversation_type": "time_request",
                "source": "time-handler",
                "success": True,
                "cached": False,
                "model_used": "simple-handler",
                "cost_estimate": 0.0
            }

        # D√©tection des salutations
        greeting_patterns = ["bonjour", "salut", "hello", "bonsoir", "bonne nuit", "hey"]
        if any(pattern in message_lower for pattern in greeting_patterns):
            current_hour = datetime.now().hour
            if current_hour < 12:
                greeting = "Bonjour"
            elif current_hour < 18:
                greeting = "Bon apr√®s-midi"
            else:
                greeting = "Bonsoir"

            html_response = f"""
            <div class="response-section medical-info">
                <h3><i class="fas fa-hand-wave"></i> Kidjamo Assistant</h3>
                <p>{greeting}! Je suis votre assistant m√©dical sp√©cialis√© dans la dr√©panocytose.</p>
                <p>Comment puis-je vous aider aujourd'hui ?</p>
                <ul class="help-list">
                    <li>ü©∫ Questions sur vos sympt√¥mes</li>
                    <li>üíä Gestion de vos m√©dicaments</li>
                    <li>üö® Conseils en cas de crise</li>
                    <li>üìö Informations sur la dr√©panocytose</li>
                </ul>
            </div>
            """

            return {
                "response": html_response,
                "conversation_type": "greeting",
                "source": "greeting-handler",
                "success": True,
                "cached": False,
                "model_used": "simple-handler",
                "cost_estimate": 0.0
            }

        # D√©tection des remerciements
        thanks_patterns = ["merci", "thank", "remercie"]
        if any(pattern in message_lower for pattern in thanks_patterns):
            html_response = """
            <div class="response-section medical-info">
                <h3><i class="fas fa-heart"></i> Kidjamo Assistant</h3>
                <p>De rien! Je suis l√† pour vous accompagner dans votre prise en charge.</p>
                <p>N'h√©sitez pas si vous avez d'autres questions sur votre sant√© ou votre traitement.</p>
            </div>
            """

            return {
                "response": html_response,
                "conversation_type": "thanks",
                "source": "thanks-handler",
                "success": True,
                "cached": False,
                "model_used": "simple-handler",
                "cost_estimate": 0.0
            }

        # D√©tection des demandes d'aide g√©n√©rale
        help_patterns = ["aide", "help", "qui es-tu", "qui es tu", "que peux-tu faire", "comment √ßa marche"]
        if any(pattern in message_lower for pattern in help_patterns):
            html_response = """
            <div class="response-section medical-info">
                <h3><i class="fas fa-robot"></i> √Ä propos de Kidjamo Assistant</h3>
                <p>Je suis votre assistant m√©dical intelligent sp√©cialis√© dans la dr√©panocytose au Cameroun.</p>
                
                <h4><i class="fas fa-stethoscope"></i> Mes capacit√©s</h4>
                <ul class="help-list">
                    <li><strong>Gestion des crises:</strong> Conseils personnalis√©s pour g√©rer la douleur</li>
                    <li><strong>Suivi m√©dicamenteux:</strong> Rappels et conseils sur vos traitements</li>
                    <li><strong>Urgences m√©dicales:</strong> Orientation vers les services d'urgence</li>
                    <li><strong>√âducation th√©rapeutique:</strong> Informations fiables sur la dr√©panocytose</li>
                </ul>
                
                <h4><i class="fas fa-exclamation-triangle"></i> Important</h4>
                <p>‚ö†Ô∏è Je ne remplace jamais un m√©decin qualifi√©. En cas d'urgence, contactez le 1510 ou rendez-vous au CHU de Yaound√©.</p>
            </div>
            """

            return {
                "response": html_response,
                "conversation_type": "help",
                "source": "help-handler",
                "success": True,
                "cached": False,
                "model_used": "simple-handler",
                "cost_estimate": 0.0
            }

        # Aucune question simple d√©tect√©e
        return None
