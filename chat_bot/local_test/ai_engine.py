"""
Moteur d'IA g√©n√©rative pour le chatbot Kidjamo
Support local (Ollama) et cloud (OpenAI/Claude) avec fallback intelligent
"""

import requests
import json
import logging
import os
from typing import Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

class AIEngine:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.claude_api_key = os.getenv('ANTHROPIC_API_KEY')

        # Configuration des mod√®les
        self.local_model = "llama3.1:8b"  # Ou "mistral:7b", "medllama2:7b"
        self.fallback_enabled = True

        # V√©rifier la disponibilit√© d'Ollama
        self.ollama_available = self._check_ollama()

    def _check_ollama(self) -> bool:
        """V√©rifie si Ollama est disponible localement"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            logger.warning("Ollama non disponible - utilisation du fallback cloud")
            return False

    def generate_response(self,
                         user_message: str,
                         context: Dict[str, Any],
                         conversation_type: str = "general") -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse intelligente bas√©e sur le contexte m√©dical
        """
        try:
            # Construire le prompt m√©dical contextualis√©
            system_prompt = self._build_medical_prompt(conversation_type, context)

            # Essayer Ollama d'abord (local)
            if self.ollama_available:
                response = self._generate_with_ollama(user_message, system_prompt, context)
                if response:
                    return response

            # Fallback vers OpenAI si disponible
            if self.openai_api_key and self.fallback_enabled:
                return self._generate_with_openai(user_message, system_prompt, context)

            # Fallback vers Claude si disponible
            if self.claude_api_key and self.fallback_enabled:
                return self._generate_with_claude(user_message, system_prompt, context)

            # Dernier fallback : r√©ponse de base
            return self._fallback_response(conversation_type)

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration IA: {e}")
            return self._fallback_response("error")

    def _build_medical_prompt(self, conversation_type: str, context: Dict[str, Any]) -> str:
        """Construit un prompt m√©dical contextualis√© pour la dr√©panocytose"""

        base_prompt = """Tu es Kidjamo Assistant, un assistant m√©dical sp√©cialis√© dans l'accompagnement des patients atteints de dr√©panocytose.

R√àGLES IMPORTANTES:
- Tu es empathique, rassurant mais prudent m√©dicalement
- Tu ne remplaces JAMAIS un m√©decin
- En cas d'urgence (douleur >7/10, difficult√©s respiratoires), tu recommandes TOUJOURS d'appeler les secours
- Tu personnalises tes r√©ponses selon l'historique de conversation
- Tu utilises un langage simple et accessible
- Tu restes dans le domaine de la dr√©panocytose
- ADAPTE ta r√©ponse au contexte conversationnel pr√©cis

NUM√âROS D'URGENCE √Ä RAPPELER:
- 115 (SAMU Cameroun)
- 112 (Urgences europ√©ennes)
- 118 (Pompiers)"""

        # Ajouter du contexte sp√©cifique selon le type de conversation
        if conversation_type == "emergency":
            base_prompt += """

CONTEXTE URGENCE CRITIQUE:
- Priorise ABSOLUMENT la s√©curit√© du patient
- Recommande l'appel imm√©diat aux secours
- Donne des conseils pratiques d'attente
- Reste calme mais ferme sur la n√©cessit√© d'aide m√©dicale
- Structure: üö® URGENCE ‚Üí Num√©ros ‚Üí Actions imm√©diates ‚Üí Informations √† communiquer"""

        elif conversation_type == "emergency_followup":
            base_prompt += """

CONTEXTE SUIVI D'URGENCE:
- Le patient a d√©j√† re√ßu des conseils d'urgence
- Il pose une question de suivi (que faire, dois-je aller aux urgences, etc.)
- R√©ponds de mani√®re SP√âCIFIQUE √† sa question
- Ne r√©p√®te PAS les m√™mes conseils d'urgence
- Adapte selon son √©tat actuel"""

        elif conversation_type == "pain_evolution":
            base_prompt += """

CONTEXTE √âVOLUTION DOULEUR:
- Le patient √©tait d√©j√† en crise douloureuse
- Il rapporte une √©volution (am√©lioration/aggravation)
- √âvalue le changement et adapte les conseils
- Si am√©lioration: encourage et surveillance
- Si aggravation: r√©√©value l'urgence"""

        elif conversation_type == "contextual_help":
            base_prompt += """

CONTEXTE AIDE CONTEXTUELLE:
- Le patient pose une question vague ("que faire?") mais il y a un contexte m√©dical actif
- Utilise le contexte (douleur, crise, urgence) pour personnaliser ta r√©ponse
- Sois SP√âCIFIQUE selon sa situation actuelle"""

        elif conversation_type == "pain":
            base_prompt += """

CONTEXTE DOULEUR:
- √âvalue le niveau de douleur (√©chelle 1-10)
- Propose des strat√©gies de gestion selon l'intensit√©
- Si >7/10 ou √©chec des antalgiques habituels = URGENCE
- Encourage la tenue d'un journal de douleur"""

        elif conversation_type == "medication":
            base_prompt += """

CONTEXTE M√âDICAMENTS:
- Rappelle l'importance de l'observance
- Explique les interactions possibles
- Encourage √† ne jamais arr√™ter sans avis m√©dical
- Propose des strat√©gies de rappel"""

        elif conversation_type == "greeting":
            base_prompt += """

CONTEXTE ACCUEIL:
- Accueille chaleureusement le patient
- Pr√©sente tes capacit√©s sp√©cialis√©es
- Propose des exemples concrets d'aide
- Reste professionnel mais bienveillant"""

        elif conversation_type == "gratitude":
            base_prompt += """

CONTEXTE REMERCIEMENT:
- Le patient te remercie
- R√©agis naturellement aux remerciements
- Rappelle ta disponibilit√©
- Termine sur une note positive et rassurante"""

        elif conversation_type == "general_help":
            base_prompt += """

CONTEXTE AIDE G√âN√âRALE:
- Question vague sans contexte m√©dical urgent
- Oriente vers les domaines d'expertise
- Propose des exemples concrets
- Encourage √† √™tre plus sp√©cifique"""

        # Ajouter l'historique r√©cent si disponible
        if context.get('recent_messages'):
            base_prompt += f"""

HISTORIQUE R√âCENT DE CONVERSATION:
{self._format_conversation_history(context['recent_messages'])}

IMPORTANT: Tiens compte de cet historique pour personnaliser ta r√©ponse et √©viter les r√©p√©titions."""

        # Ajouter le contexte de session
        if context.get('pain_level'):
            base_prompt += f"\n\nDouleur actuelle rapport√©e: {context['pain_level']}/10"

        if context.get('current_crisis'):
            base_prompt += "\n\nPatient actuellement en crise - surveillance renforc√©e"

        if context.get('emergency_context'):
            base_prompt += "\n\nCONTEXTE D'URGENCE ACTIF - Le patient a d√©j√† re√ßu des conseils d'urgence"

        if context.get('pain_evolution'):
            evolution = context['pain_evolution']
            base_prompt += f"\n\n√âvolution douleur: {evolution['previous']}/10 ‚Üí {evolution['current']}/10 ({evolution['trend']})"

        return base_prompt

    def _format_conversation_history(self, messages: list) -> str:
        """Formate l'historique pour le contexte"""
        if not messages or len(messages) == 0:
            return "Aucun historique"

        formatted = []
        for msg in messages[-3:]:  # Derniers 3 messages
            role = "Patient" if msg.get('role') == 'user' else "Assistant"
            content = msg.get('content', '')[:100]  # Limite √† 100 caract√®res
            formatted.append(f"- {role}: {content}")

        return '\n'.join(formatted)

    def _generate_with_ollama(self, user_message: str, system_prompt: str, context: Dict) -> Optional[Dict]:
        """G√©n√©ration avec Ollama (local)"""
        try:
            payload = {
                "model": self.local_model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 500
                }
            }

            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                ai_response = result.get('message', {}).get('content', '')

                return {
                    'success': True,
                    'response': ai_response.strip(),
                    'source': 'ollama_local',
                    'model': self.local_model,
                    'conversation_type': context.get('conversation_type', 'general')
                }
        except Exception as e:
            logger.error(f"Erreur Ollama: {e}")

        return None

    def _generate_with_openai(self, user_message: str, system_prompt: str, context: Dict) -> Dict:
        """G√©n√©ration avec OpenAI GPT"""
        try:
            import openai

            client = openai.OpenAI(api_key=self.openai_api_key)

            response = client.chat.completions.create(
                model="gpt-4o-mini",  # Plus √©conomique
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=500,
                temperature=0.7
            )

            return {
                'success': True,
                'response': response.choices[0].message.content.strip(),
                'source': 'openai_cloud',
                'model': 'gpt-4o-mini',
                'conversation_type': context.get('conversation_type', 'general')
            }

        except Exception as e:
            logger.error(f"Erreur OpenAI: {e}")
            return self._fallback_response("error")

    def _generate_with_claude(self, user_message: str, system_prompt: str, context: Dict) -> Dict:
        """G√©n√©ration avec Anthropic Claude"""
        try:
            import anthropic

            client = anthropic.Anthropic(api_key=self.claude_api_key)

            message = client.messages.create(
                model="claude-3-haiku-20240307",  # Plus rapide et √©conomique
                max_tokens=500,
                temperature=0.7,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_message}
                ]
            )

            return {
                'success': True,
                'response': message.content[0].text.strip(),
                'source': 'claude_cloud',
                'model': 'claude-3-haiku',
                'conversation_type': context.get('conversation_type', 'general')
            }

        except Exception as e:
            logger.error(f"Erreur Claude: {e}")
            return self._fallback_response("error")

    def _fallback_response(self, conversation_type: str) -> Dict:
        """R√©ponses de fallback contextualis√©es"""
        fallbacks = {
            "emergency": {
                'response': """üö® URGENCE M√âDICALE D√âTECT√âE

Je ne peux pas g√©n√©rer de r√©ponse personnalis√©e actuellement, mais votre s√©curit√© est prioritaire :

APPELEZ IMM√âDIATEMENT:
- 115 (SAMU Cameroun)
- 112 (Urgences europ√©ennes)

EN ATTENDANT LES SECOURS:
- Restez calme
- Ne bougez pas si possible
- Pr√©parez vos papiers d'identit√©
- Mentionnez "patient dr√©panocytaire"

‚ö†Ô∏è Cette situation n√©cessite une prise en charge m√©dicale imm√©diate.""",
                'conversation_type': 'emergency'
            },
            "greeting": {
                'response': """Bonjour ! Je suis votre assistant Kidjamo, sp√©cialis√© dans l'accompagnement des patients dr√©panocytaires.

Je peux vous aider avec :
ü©∫ Gestion de la douleur
üíä Questions sur vos m√©dicaments  
üö® Situations d'urgence
üìö Informations sur la dr√©panocytose

Comment puis-je vous accompagner aujourd'hui ?""",
                'conversation_type': 'greeting'
            },
            "error": {
                'response': """Je rencontre actuellement des difficult√©s techniques pour g√©n√©rer une r√©ponse personnalis√©e.

Cependant, je reste disponible pour vous aider. Pouvez-vous reformuler votre question ?

Si c'est urgent, n'h√©sitez pas √† contacter directement les services m√©dicaux.""",
                'conversation_type': 'error'
            }
        }

        return {
            'success': True,
            'response': fallbacks.get(conversation_type, fallbacks['error'])['response'],
            'source': 'fallback_local',
            'conversation_type': conversation_type
        }

# Instance globale
ai_engine = AIEngine()
