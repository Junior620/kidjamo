#!/usr/bin/env python3
"""
Script d'installation et de configuration pour l'IA g√©n√©rative Kidjamo
Installe Ollama et configure l'environnement automatiquement
"""

import os
import sys
import subprocess
import platform
import requests
import time
from pathlib import Path

def print_step(message):
    print(f"\nüîß {message}")
    print("=" * 50)

def print_success(message):
    print(f"‚úÖ {message}")

def print_warning(message):
    print(f"‚ö†Ô∏è  {message}")

def print_error(message):
    print(f"‚ùå {message}")

def check_python_version():
    """V√©rifie que Python 3.8+ est install√©"""
    if sys.version_info < (3, 8):
        print_error("Python 3.8 ou sup√©rieur requis")
        sys.exit(1)
    print_success(f"Python {sys.version.split()[0]} d√©tect√©")

def install_python_dependencies():
    """Installe les d√©pendances Python"""
    print_step("Installation des d√©pendances Python")

    try:
        subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", "pip"], check=True)
        subprocess.run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"], check=True)
        print_success("D√©pendances Python install√©es")
    except subprocess.CalledProcessError as e:
        print_error(f"Erreur installation d√©pendances: {e}")
        return False
    return True

def install_ollama():
    """Installe Ollama selon l'OS"""
    print_step("Installation d'Ollama (IA locale)")

    os_name = platform.system().lower()

    if os_name == "windows":
        print("üì• T√©l√©chargement d'Ollama pour Windows...")
        print("Visitez: https://ollama.ai/download/windows")
        print("Ou ex√©cutez: winget install Ollama.Ollama")

    elif os_name == "darwin":  # macOS
        try:
            subprocess.run(["brew", "install", "ollama"], check=True)
            print_success("Ollama install√© via Homebrew")
        except subprocess.CalledProcessError:
            print_warning("Homebrew non trouv√©. Installation manuelle requise:")
            print("Visitez: https://ollama.ai/download/mac")

    elif os_name == "linux":
        try:
            # Installation via le script officiel
            subprocess.run([
                "curl", "-fsSL", "https://ollama.ai/install.sh"
            ], stdout=subprocess.PIPE, check=True)
            print_success("Ollama install√© sur Linux")
        except subprocess.CalledProcessError:
            print_warning("Installation automatique √©chou√©e. Utilisez:")
            print("curl -fsSL https://ollama.ai/install.sh | sh")

    return True

def start_ollama_service():
    """D√©marre le service Ollama"""
    print_step("D√©marrage du service Ollama")

    try:
        # V√©rifier si Ollama est d√©j√† en cours
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print_success("Ollama d√©j√† en cours d'ex√©cution")
            return True
    except requests.RequestException:
        pass

    # D√©marrer Ollama
    os_name = platform.system().lower()

    if os_name == "windows":
        print("D√©marrez Ollama via le menu D√©marrer ou:")
        print("ollama serve")
    else:
        try:
            subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            time.sleep(3)  # Laisser le temps au service de d√©marrer
            print_success("Service Ollama d√©marr√©")
        except FileNotFoundError:
            print_error("Ollama non trouv√©. Installez-le d'abord.")
            return False

    return True

def download_ai_models():
    """T√©l√©charge les mod√®les d'IA recommand√©s"""
    print_step("T√©l√©chargement des mod√®les d'IA locaux")

    models = [
        ("llama3.1:8b", "Mod√®le principal (4.7GB)"),
        ("mistral:7b", "Alternative l√©g√®re (4.1GB)"),
    ]

    for model, description in models:
        print(f"üì¶ T√©l√©chargement de {model} - {description}")
        try:
            result = subprocess.run(
                ["ollama", "pull", model],
                capture_output=True,
                text=True,
                timeout=1800  # 30 minutes max
            )

            if result.returncode == 0:
                print_success(f"Mod√®le {model} t√©l√©charg√©")
            else:
                print_warning(f"√âchec t√©l√©chargement {model}: {result.stderr}")

        except subprocess.TimeoutExpired:
            print_warning(f"Timeout pour {model} - continuez manuellement avec: ollama pull {model}")
        except FileNotFoundError:
            print_error("Ollama non trouv√©. Installez-le d'abord.")
            return False

    return True

def setup_environment():
    """Configure le fichier d'environnement"""
    print_step("Configuration de l'environnement")

    env_file = Path(".env")
    env_example = Path(".env.example")

    if not env_file.exists() and env_example.exists():
        # Copier l'exemple
        with open(env_example, 'r') as f:
            content = f.read()

        with open(env_file, 'w') as f:
            f.write(content)

        print_success("Fichier .env cr√©√© √† partir de l'exemple")
        print_warning("Configurez vos cl√©s API dans le fichier .env si vous voulez utiliser les services cloud")
    else:
        print_success("Fichier .env d√©j√† pr√©sent")

    return True

def test_ai_setup():
    """Teste la configuration IA"""
    print_step("Test de la configuration IA")

    try:
        # Test Ollama local
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            if models:
                print_success(f"Ollama fonctionnel avec {len(models)} mod√®le(s)")
                for model in models[:3]:  # Afficher les 3 premiers
                    print(f"  - {model.get('name', 'Unknown')}")
            else:
                print_warning("Ollama fonctionne mais aucun mod√®le t√©l√©charg√©")
        else:
            print_warning("Ollama non accessible")
    except requests.RequestException:
        print_warning("Ollama non disponible - les APIs cloud seront utilis√©es en fallback")

    # Test des imports Python
    try:
        from ai_engine import ai_engine
        print_success("Module AI Engine import√© avec succ√®s")

        # Test de base
        test_response = ai_engine.generate_response(
            "Test de configuration",
            {"session_id": "test"},
            "general"
        )

        if test_response.get('success'):
            source = test_response.get('source', 'unknown')
            print_success(f"Test IA r√©ussi via {source}")
        else:
            print_warning("Test IA √©chou√© mais configuration OK")

    except ImportError as e:
        print_error(f"Erreur import AI Engine: {e}")
        return False

    return True

def main():
    """Script principal d'installation"""
    print("üöÄ INSTALLATION IA G√âN√âRATIVE KIDJAMO")
    print("=" * 50)

    # 1. V√©rifications pr√©liminaires
    check_python_version()

    # 2. Installation des d√©pendances
    if not install_python_dependencies():
        print_error("Installation des d√©pendances √©chou√©e")
        sys.exit(1)

    # 3. Installation Ollama
    install_ollama()

    # 4. D√©marrage du service
    if start_ollama_service():
        # 5. T√©l√©chargement des mod√®les
        download_ai_models()

    # 6. Configuration environnement
    setup_environment()

    # 7. Tests finaux
    test_ai_setup()

    print("\nüéâ INSTALLATION TERMIN√âE !")
    print("=" * 50)
    print("‚úÖ Votre chatbot Kidjamo est maintenant √©quip√© d'IA g√©n√©rative")
    print("\nüìã Prochaines √©tapes :")
    print("1. Configurez vos cl√©s API dans .env (optionnel)")
    print("2. D√©marrez le serveur: python chatbot_server.py")
    print("3. Testez sur http://localhost:5000")
    print("\nüí° Conseil : Les mod√®les locaux peuvent prendre du temps √† t√©l√©charger")
    print("    En attendant, le syst√®me utilisera les r√©ponses de fallback")

if __name__ == "__main__":
    main()
