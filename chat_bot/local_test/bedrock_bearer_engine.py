"""
Bedrock AI Engine adapt√© pour Bearer Token Authentication
Version optimis√©e pour votre cl√© API Bedrock sp√©cifique
"""

import requests
import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import time

logger = logging.getLogger(__name__)

class BedrockBearerTokenEngine:
    """Moteur Bedrock avec authentification Bearer Token"""

    def __init__(self):
        # Configuration avec votre token Bearer
        self.bearer_token = os.getenv('AWS_BEARER_TOKEN_BEDROCK')
        self.api_endpoint = os.getenv('BEDROCK_API_ENDPOINT', 'https://bedrock-runtime.us-east-1.amazonaws.com')

        if not self.bearer_token:
            raise ValueError("AWS_BEARER_TOKEN_BEDROCK non trouv√© dans les variables d'environnement")

        # Configuration des mod√®les Bedrock disponibles
        self.available_models = {
            "claude-3-haiku": {
                "id": "anthropic.claude-3-haiku-20240307-v1:0",
                "max_tokens": 4096,
                "cost_per_1k_input": 0.00025,
                "cost_per_1k_output": 0.00125,
                "recommended": True,
                "use_case": "Urgences m√©dicales - Rapide et pr√©cis"
            },
            "claude-3-sonnet": {
                "id": "anthropic.claude-3-sonnet-20240229-v1:0",
                "max_tokens": 4096,
                "cost_per_1k_input": 0.003,
                "cost_per_1k_output": 0.015,
                "recommended": False,
                "use_case": "Analyses complexes - Qualit√© sup√©rieure"
            },
            "claude-v2": {
                "id": "anthropic.claude-v2",
                "max_tokens": 8192,
                "cost_per_1k_input": 0.008,
                "cost_per_1k_output": 0.024,
                "recommended": False,
                "use_case": "Conversations longues"
            },
            "titan-text": {
                "id": "amazon.titan-text-express-v1",
                "max_tokens": 8192,
                "cost_per_1k_input": 0.0008,
                "cost_per_1k_output": 0.0016,
                "recommended": True,
                "use_case": "Questions g√©n√©rales √©conomiques"
            }
        }

        # Mod√®le par d√©faut optimis√© pour m√©dical
        self.primary_model = os.getenv('BEDROCK_PRIMARY_MODEL', 'claude-3-haiku')
        self.fallback_model = os.getenv('BEDROCK_FALLBACK_MODEL', 'titan-text')

        # Configuration performance
        self.request_timeout = int(os.getenv('BEDROCK_TIMEOUT', 30))
        self.max_retries = int(os.getenv('BEDROCK_MAX_RETRIES', 3))
        self.conversation_contexts = {}

        # Cache et m√©triques
        self.response_cache = {}
        self.cache_ttl = int(os.getenv('BEDROCK_CACHE_TTL', 1800))
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'cost_estimate': 0.0,
            'models_used': {}
        }

        logger.info(f"BedrockBearerTokenEngine initialis√© avec mod√®le principal: {self.primary_model}")
        logger.info(f"Endpoint: {self.api_endpoint}")

    def process_message_with_ai(self, user_message: str, context: Dict) -> Dict:
        """Traite le message avec Bedrock via Bearer Token"""

        session_id = context.get('session_id', 'default')
        self.metrics['total_requests'] += 1

        try:
            # V√©rification du cache
            cache_key = self._get_cache_key(user_message, session_id)
            cached_response = self._get_from_cache(cache_key)
            if cached_response:
                logger.info(f"R√©ponse servie depuis le cache pour session {session_id}")
                return cached_response

            # S√©lection du mod√®le selon l'urgence
            model_name = self._select_model_for_urgency(user_message, context)
            model_config = self.available_models[model_name]

            # Construire le prompt m√©dical sp√©cialis√©
            system_prompt = self._build_medical_prompt(user_message, session_id, context)

            # G√©n√©rer la r√©ponse avec Bedrock
            response = self._generate_with_bedrock_api(
                model_config["id"],
                system_prompt,
                user_message,
                model_config
            )

            if response:
                # Construire la r√©ponse finale
                final_response = {
                    "response": self._format_response_as_html(response["content"], user_message),
                    "conversation_type": self._detect_conversation_type(user_message),
                    "source": "bedrock-bearer-token",
                    "model_used": model_name,
                    "model_id": model_config["id"],
                    "raw_response": response["content"],
                    "success": True,
                    "cached": False,
                    "cost_estimate": response["cost"],
                    "tokens_used": response.get("tokens", {})
                }

                # Stocker en cache
                self._store_in_cache(cache_key, final_response)

                # Sauvegarder le contexte
                self._save_conversation_context(session_id, user_message, response["content"])

                # Mettre √† jour les m√©triques
                self.metrics['successful_requests'] += 1
                self.metrics['cost_estimate'] += response["cost"]
                self.metrics['models_used'][model_name] = self.metrics['models_used'].get(model_name, 0) + 1

                logger.info(f"R√©ponse Bedrock g√©n√©r√©e avec {model_name} pour session {session_id}")
                return final_response
            else:
                return self._fallback_response(user_message, "bedrock_api_error")

        except Exception as e:
            logger.error(f"Erreur critique Bedrock Bearer Token: {e}")
            self.metrics['failed_requests'] += 1
            return self._fallback_response(user_message, "system_error")

    def _select_model_for_urgency(self, user_message: str, context: Dict) -> str:
        """S√©lectionne le mod√®le optimal selon l'urgence m√©dicale"""

        urgency_level = context.get('urgency_level', 'normal')
        message_lower = user_message.lower()

        # Mots-cl√©s d'urgence critique
        critical_keywords = ["poitrine", "respir", "8/10", "9/10", "10/10", "insupportable", "mourir"]

        # Pour les urgences m√©dicales critiques ‚Üí Claude Haiku (rapidit√© + pr√©cision)
        if urgency_level == 'critical' or any(word in message_lower for word in critical_keywords):
            return "claude-3-haiku"

        # Pour les questions sur m√©dicaments ‚Üí Claude Haiku (s√©curit√© m√©dicale)
        elif any(word in message_lower for word in ["m√©dicament", "siklos", "traitement", "dose"]):
            return "claude-3-haiku"

        # Pour les questions g√©n√©rales ‚Üí Titan (√©conomique)
        else:
            return "titan-text"

    def _generate_with_bedrock_api(self, model_id: str, system_prompt: str, user_message: str, model_config: Dict) -> Optional[Dict]:
        """G√©n√®re une r√©ponse via l'API Bedrock avec Bearer Token"""

        try:
            # URL de l'endpoint Bedrock
            url = f"{self.api_endpoint}/model/{model_id}/invoke"

            # Headers avec Bearer Token
            headers = {
                "Authorization": f"Bearer {self.bearer_token}",
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "Kidjamo-HealthBot-Bedrock/1.0"
            }

            # Construire le payload selon le type de mod√®le
            if "claude" in model_id:
                payload = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": min(model_config["max_tokens"], 800),
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "system": system_prompt,
                    "messages": [
                        {
                            "role": "user",
                            "content": user_message
                        }
                    ]
                }
            elif "titan" in model_id:
                full_prompt = f"{system_prompt}\n\nQuestion: {user_message}\nR√©ponse m√©dicale:"
                payload = {
                    "inputText": full_prompt,
                    "textGenerationConfig": {
                        "maxTokenCount": min(model_config["max_tokens"], 700),
                        "temperature": 0.3,
                        "topP": 0.8
                    }
                }
            else:
                raise ValueError(f"Type de mod√®le non support√©: {model_id}")

            # Appel API avec retry
            response = self._make_api_call_with_retry(url, headers, payload)

            if response and response.status_code == 200:
                response_data = response.json()

                # Parser la r√©ponse selon le type de mod√®le
                parsed_response = self._parse_model_response(model_id, response_data)

                # Calculer le co√ªt estim√©
                cost = self._calculate_cost(model_config, parsed_response.get("tokens", {}))

                return {
                    "content": parsed_response["content"],
                    "cost": cost,
                    "tokens": parsed_response.get("tokens", {})
                }
            else:
                error_msg = f"Erreur API Bedrock: {response.status_code if response else 'Timeout'}"
                logger.error(error_msg)
                return None

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration Bedrock: {e}")
            return None

    def _make_api_call_with_retry(self, url: str, headers: Dict, payload: Dict) -> Optional[requests.Response]:
        """Effectue l'appel API Bedrock avec retry automatique"""

        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    url,
                    headers=headers,
                    json=payload,
                    timeout=self.request_timeout
                )

                return response

            except requests.exceptions.Timeout:
                logger.warning(f"Timeout API Bedrock - Tentative {attempt + 1}/{self.max_retries}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)

            except requests.exceptions.RequestException as e:
                logger.error(f"Erreur r√©seau Bedrock - Tentative {attempt + 1}/{self.max_retries}: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)

        return None

    def _parse_model_response(self, model_id: str, response: Dict) -> Dict:
        """Parse la r√©ponse selon le type de mod√®le avec gestion robuste des formats"""

        try:
            logger.info(f"Debug - R√©ponse brute Bedrock: {json.dumps(response, indent=2)[:500]}...")

            # Essayer diff√©rents formats de r√©ponse Claude
            if "claude" in model_id:
                content = None
                tokens = {"input": 0, "output": 0}

                # Format 1: Standard Bedrock Claude
                if "content" in response and isinstance(response["content"], list):
                    content = response["content"][0]["text"]
                    if "usage" in response:
                        tokens = {
                            "input": response["usage"].get("input_tokens", 0),
                            "output": response["usage"].get("output_tokens", 0)
                        }

                # Format 2: Direct text response
                elif "completion" in response:
                    content = response["completion"]

                # Format 3: Message format
                elif "message" in response:
                    content = response["message"]

                # Format 4: Text field direct
                elif "text" in response:
                    content = response["text"]

                # Format 5: R√©ponse dans un wrapper
                elif "response" in response:
                    content = response["response"]

                # üîß NOUVEAU: Format avec liste ['Output', 'Version']
                elif isinstance(response, list) and len(response) >= 2:
                    # Si on re√ßoit ['Output', 'Version'], prendre le premier √©l√©ment
                    content = str(response[0])
                    logger.info(f"Format liste d√©tect√©: {response}")

                # üîß NOUVEAU: Format avec cl√©s sp√©cifiques
                elif "Output" in response:
                    content = response["Output"]
                elif "output" in response:
                    content = response["output"]

                if not content:
                    # Derni√®re tentative: chercher n'importe quel champ text
                    for key, value in response.items():
                        if isinstance(value, str) and len(value) > 10:
                            content = value
                            break

                if not content:
                    # üîß NOUVEAU: Si format compl√®tement inattendu, g√©n√©rer r√©ponse de fallback
                    logger.warning(f"Format Bedrock inattendu: {response}")
                    content = "Bonjour ! Je suis votre assistant Kidjamo sp√©cialis√© dans la dr√©panocytose. Comment puis-je vous aider aujourd'hui ?"

            # Essayer diff√©rents formats de r√©ponse Titan
            elif "titan" in model_id:
                content = None
                tokens = {"input": 0, "output": 0}

                # Format 1: Standard Titan
                if "results" in response and isinstance(response["results"], list):
                    content = response["results"][0]["outputText"]
                    tokens = {
                        "input": response.get("inputTextTokenCount", 0),
                        "output": response["results"][0].get("tokenCount", 0)
                    }

                # Format 2: Direct outputText
                elif "outputText" in response:
                    content = response["outputText"]

                # üîß NOUVEAU: Format Titan avec liste ['Output', 'Version']
                elif isinstance(response, list) and len(response) >= 2:
                    content = str(response[0])
                    logger.info(f"Format Titan liste d√©tect√©: {response}")

                # üîß NOUVEAU: Format avec cl√©s sp√©cifiques Titan
                elif "Output" in response:
                    content = response["Output"]
                elif "output" in response:
                    content = response["output"]

                if not content:
                    # üîß NOUVEAU: Fallback pour Titan aussi
                    logger.warning(f"Format Titan inattendu: {response}")
                    content = "Bonjour ! Je suis votre assistant Kidjamo. Pour toute question sur la dr√©panocytose, je suis l√† pour vous aider !"

            else:
                content = f"Mod√®le {model_id} non support√©"

            # üîß VALIDATION FINALE du contenu
            if not content or len(content.strip()) < 5:
                content = "Bonjour ! Je suis Kidjamo Assistant, votre compagnon sant√© sp√©cialis√© dans la dr√©panocytose. Comment puis-je vous accompagner aujourd'hui ?"

            return {
                "content": content.strip() if content else "R√©ponse vide de Bedrock",
                "tokens": tokens
            }

        except Exception as e:
            logger.error(f"Erreur parsing r√©ponse {model_id}: {e}")
            logger.error(f"R√©ponse probl√©matique: {response}")
            # üîß FALLBACK ROBUSTE en cas d'erreur critique
            return {
                "content": "Bonjour ! Je suis votre assistant Kidjamo sp√©cialis√© dans l'accompagnement des patients atteints de dr√©panocytose. Comment puis-je vous aider aujourd'hui ? En cas d'urgence m√©dicale, contactez le 1510 ou rendez-vous au CHU Yaound√©.",
                "tokens": {"input": 0, "output": 0}
            }

    def _calculate_cost(self, model_config: Dict, tokens: Dict) -> float:
        """Calcule le co√ªt estim√© de la requ√™te"""
        try:
            input_tokens = tokens.get("input", 0)
            output_tokens = tokens.get("output", 0)

            input_cost = (input_tokens / 1000) * model_config["cost_per_1k_input"]
            output_cost = (output_tokens / 1000) * model_config["cost_per_1k_output"]

            return round(input_cost + output_cost, 6)
        except:
            return 0.0

    def _build_medical_prompt(self, user_message: str, session_id: str, context: Dict) -> str:
        """Construit un prompt m√©dical contextualis√© pour Bedrock"""

        patient_info = context.get('patient_info', {})
        age = patient_info.get('age', 'Non sp√©cifi√©')

        base_prompt = f"""Tu es Kidjamo Assistant, un assistant m√©dical IA sp√©cialis√© dans l'accompagnement des patients atteints de dr√©panocytose au Cameroun.

CONTEXTE PATIENT:
- √Çge: {age}
- Session: {session_id}
- Niveau d'urgence: {context.get('urgency_level', 'normal')}

PROTOCOLE M√âDICAL STRICT:
- Tu ne remplaces JAMAIS un m√©decin qualifi√©
- URGENCE VITALE si: douleur >7/10, difficult√©s respiratoires, fi√®vre >38.5¬∞C
- En urgence: directive IMM√âDIATE vers services d'urgence camerounais
- Domaine STRICT: dr√©panocytose uniquement
- R√©ponds en fran√ßais simple et accessible

SERVICES D'URGENCE CAMEROUN:
- 1510 (Urgence nationale Cameroun)
- H√¥pital Central Yaound√© - Urgences
- H√¥pital G√©n√©ral Douala - Service d'urgence
- CHU Yaound√© - Centre r√©f√©rence dr√©panocytose

CENTRES SP√âCIALIS√âS:
- CHU Yaound√© - H√©matologie sp√©cialis√©e
- H√¥pital Laquintinie Douala - Suivi dr√©panocytose
- Centre Pasteur Cameroun - Expertise dr√©panocytose

INSTRUCTIONS DE R√âPONSE:
- Sois empathique et rassurant mais prudent m√©dicalement
- Structure avec des √©mojis (üö® ü©∫ üíä) pour clarifier
- Si urgence: priorise ABSOLUMENT la s√©curit√© du patient
- Donne des conseils pratiques et pr√©cis adapt√©s au Cameroun
- Termine par une question de suivi si appropri√©"""

        # Ajouter contexte conversation r√©cente
        if session_id in self.conversation_contexts:
            recent = self.conversation_contexts[session_id][-2:]
            if recent:
                base_prompt += "\n\nCONTEXTE CONVERSATION R√âCENTE:\n"
                for ctx in recent:
                    base_prompt += f"Patient: {ctx['user']}\nAssistant: {ctx['bot'][:100]}...\n"

        return base_prompt

    def _detect_conversation_type(self, message: str) -> str:
        """D√©tecte le type de conversation"""
        message_lower = message.lower()

        critical_keywords = ["mal", "poitrine", "respir", "aide", "urgent", "grave", "intense", "8/10", "9/10", "10/10"]
        pain_keywords = ["douleur", "mal", "/10", "souffre", "crise", "insupportable"]
        medication_keywords = ["m√©dicament", "siklos", "traitement", "pilule", "dose", "oubli"]

        if any(keyword in message_lower for keyword in critical_keywords):
            return "emergency"
        elif any(keyword in message_lower for keyword in pain_keywords):
            return "pain_management"
        elif any(keyword in message_lower for keyword in medication_keywords):
            return "medication"
        else:
            return "general"

    def _format_response_as_html(self, ai_response: str, user_message: str) -> str:
        """Formate la r√©ponse IA en HTML s√©curis√©"""

        # D√©tection d'urgence
        is_emergency = any(word in user_message.lower() for word in ["mal", "poitrine", "respir", "aide", "urgent"])

        if is_emergency:
            css_class = "response-section emergency-alert"
            icon = "fas fa-exclamation-triangle"
        else:
            css_class = "response-section medical-info"
            icon = "fas fa-user-md"

        # Nettoyage s√©curis√© du contenu
        import html
        safe_response = html.escape(ai_response)

        # Conversion markdown basique
        safe_response = safe_response.replace('\n\n', '</p><p>')
        safe_response = safe_response.replace('**', '<strong>').replace('**', '</strong>')

        html_response = f"""
        <div class="{css_class}">
            <h3><i class="{icon}"></i> Kidjamo Assistant</h3>
            <p>{safe_response}</p>
            <div class="bedrock-footer">
                <small>R√©ponse g√©n√©r√©e par Amazon Bedrock - Ne remplace pas un avis m√©dical</small>
            </div>
        </div>
        """

        return html_response

    def _get_cache_key(self, user_message: str, session_id: str) -> str:
        """G√©n√®re une cl√© de cache"""
        return f"{session_id}:{hash(user_message.lower().strip())}"

    def _get_from_cache(self, cache_key: str) -> Optional[Dict]:
        """R√©cup√®re une r√©ponse du cache si disponible"""
        if cache_key in self.response_cache:
            cached_data = self.response_cache[cache_key]
            if datetime.now().timestamp() - cached_data['timestamp'] < self.cache_ttl:
                self.metrics['cache_hits'] += 1
                return cached_data['response']
        return None

    def _store_in_cache(self, cache_key: str, response: Dict):
        """Stocke une r√©ponse dans le cache"""
        self.response_cache[cache_key] = {
            'response': response,
            'timestamp': datetime.now().timestamp()
        }

    def _save_conversation_context(self, session_id: str, user_message: str, ai_response: str):
        """Sauvegarde le contexte de conversation"""
        if session_id not in self.conversation_contexts:
            self.conversation_contexts[session_id] = []

        self.conversation_contexts[session_id].append({
            "user": user_message,
            "bot": ai_response,
            "timestamp": datetime.now().isoformat()
        })

        # Limiter √† 10 √©changes par session
        if len(self.conversation_contexts[session_id]) > 10:
            self.conversation_contexts[session_id] = self.conversation_contexts[session_id][-10:]

    def _fallback_response(self, user_message: str, error_type: str) -> Dict:
        """R√©ponses de secours si Bedrock √©choue"""

        message_lower = user_message.lower()

        if any(word in message_lower for word in ["mal", "poitrine", "respir", "aide", "urgent"]):
            html_response = """
            <div class="response-section emergency-alert">
                <h3><i class="fas fa-ambulance"></i> PROTOCOLE D'URGENCE ACTIV√â</h3>
                <p><strong>Douleur thoracique ou difficult√©s respiratoires d√©tect√©es</strong></p>
                <ul class="urgent-list">
                    <li><strong>APPELEZ IMM√âDIATEMENT:</strong></li>
                    <li><span class="emergency-number">1510</span> Urgence nationale Cameroun</li>
                    <li><strong>CHU Yaound√©</strong> - Centre sp√©cialis√© dr√©panocytose</li>
                    <li><strong>H√¥pital Central Yaound√©</strong> - Service urgences</li>
                </ul>
                <p>‚ö†Ô∏è <strong>Mentionnez "patient dr√©panocytaire"</strong></p>
                <p>Actions imm√©diates: restez calme, position confortable, documents m√©dicaux pr√™ts</p>
            </div>
            """
        else:
            html_response = """
            <div class="response-section medical-info">
                <h3><i class="fas fa-user-md"></i> Assistant Kidjamo</h3>
                <p>Service m√©dical sp√©cialis√© dr√©panocytose Cameroun:</p>
                <ul class="help-list">
                    <li>ü©∫ Gestion crises douloureuses</li>
                    <li>üíä Suivi m√©dicamenteux (Siklos, antalgiques)</li>
                    <li>üö® Protocoles urgence dr√©panocytose</li>
                    <li>üìö √âducation th√©rapeutique</li>
                </ul>
                <p>Reformulez votre question pour une assistance personnalis√©e</p>
            </div>
            """

        return {
            "response": html_response,
            "conversation_type": "fallback",
            "source": f"bedrock-fallback-{error_type}",
            "success": True,
            "cached": False
        }

    def get_bedrock_metrics(self) -> Dict[str, Any]:
        """M√©triques d√©taill√©es pour monitoring Bedrock"""

        # Calcul des co√ªts par mod√®le
        cost_breakdown = {}
        for model_name, usage_count in self.metrics['models_used'].items():
            model_config = self.available_models[model_name]
            estimated_cost = usage_count * 0.002  # Estimation moyenne par requ√™te
            cost_breakdown[model_name] = {
                "requests": usage_count,
                "estimated_cost": estimated_cost,
                "cost_per_request": estimated_cost / max(usage_count, 1)
            }

        return {
            "status": "operational",
            "service": "bedrock-bearer-token",
            "endpoint": self.api_endpoint,
            "primary_model": self.primary_model,
            "fallback_model": self.fallback_model,
            "performance": {
                "total_requests": self.metrics['total_requests'],
                "successful_requests": self.metrics['successful_requests'],
                "failed_requests": self.metrics['failed_requests'],
                "success_rate": (self.metrics['successful_requests'] / max(self.metrics['total_requests'], 1)) * 100,
                "cache_hit_rate": (self.metrics['cache_hits'] / max(self.metrics['total_requests'], 1)) * 100
            },
            "costs": {
                "total_estimated": self.metrics['cost_estimate'],
                "average_per_request": self.metrics['cost_estimate'] / max(self.metrics['total_requests'], 1),
                "breakdown_by_model": cost_breakdown
            },
            "models_usage": self.metrics['models_used'],
            "cache_size": len(self.response_cache),
            "active_sessions": len(self.conversation_contexts)
        }

    def get_available_models(self) -> Dict[str, Any]:
        """Retourne la liste des mod√®les disponibles avec leurs caract√©ristiques"""
        return {
            "models": self.available_models,
            "current_primary": self.primary_model,
            "current_fallback": self.fallback_model,
            "selection_strategy": "Automatique selon urgence m√©dicale"
        }
