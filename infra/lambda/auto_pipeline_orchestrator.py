                        '--input_path': f's3://{self.bucket_name}/raw/',
                        '--quarantine_output_path': f's3://{self.bucket_name}/quarantine/',
                        '--bronze_output_path': f's3://{self.bucket_name}/bronze/',
                        '--raw_input_path': f's3://{self.bucket_name}/raw/'
                    })
                elif job_config['name'] == 'kidjamo-dev-bronze-to-silver':
                    job_params.update({
                        '--input_path': f's3://{self.bucket_name}/bronze/',
                        '--bronze_input_path': f's3://{self.bucket_name}/bronze/',
                        '--silver_output_path': f's3://{self.bucket_name}/silver/'
                    })
                elif job_config['name'] == 'kidjamo-dev-silver-to-gold':
                    job_params.update({
                        '--input_path': f's3://{self.bucket_name}/silver/',
                        '--silver_input_path': f's3://{self.bucket_name}/silver/',
                        '--gold_output_path': f's3://{self.bucket_name}/gold/',
                        '--rds_secret_name': 'kidjamo-dev-rds-credentials'
                    })
#!/usr/bin/env python3
"""
Fonction Lambda d'orchestration automatique - Pipeline MPU Christian
D√©clenche automatiquement tous les jobs de la pipeline quand de nouvelles donn√©es arrivent
"""

import json
import boto3
import logging
from datetime import datetime, timezone
from typing import Dict, Any

logger = logging.getLogger()
logger.setLevel(logging.INFO)

class AutoPipelineOrchestrator:
    """Orchestrateur automatique de la pipeline MPU Christian"""

    def __init__(self):
        self.region = "eu-west-1"
        self.s3_client = boto3.client('s3', region_name=self.region)
        self.glue_client = boto3.client('glue', region_name=self.region)
        self.sns_client = boto3.client('sns', region_name=self.region)

        # Configuration pipeline
        self.bucket_name = "kidjamo-dev-datalake-e75d5213"
        self.jobs = [
            "kidjamo-dev-raw-to-bronze",      # Nom correct du job existant
            "kidjamo-dev-bronze-to-silver",   # Nom correct du job existant
            "kidjamo-dev-silver-to-gold"      # Nom correct du job existant
        ]

    def lambda_handler(self, event, context):
        """Point d'entr√©e Lambda - d√©clench√© par S3 ou EventBridge"""

        logger.info("üöÄ D√âCLENCHEMENT AUTO PIPELINE MPU CHRISTIAN")
        logger.info(f"Event: {json.dumps(event, indent=2)}")

        try:
            # Identifier le type de d√©clencheur
            trigger_type = self._identify_trigger(event)

            if trigger_type == "s3_raw":
                # Nouvelles donn√©es dans S3 raw ‚Üí d√©clencher pipeline compl√®te
                return self._handle_new_raw_data(event)

            elif trigger_type == "schedule":
                # D√©clenchement programm√© ‚Üí traitement batch p√©riodique
                return self._handle_scheduled_processing()

            elif trigger_type == "manual":
                # D√©clenchement manuel ‚Üí traitement imm√©diat
                return self._handle_manual_trigger(event)

            elif trigger_type == "batch_ready":
                # D√©clenchement par lot haute fr√©quence
                return self._handle_new_raw_data(event)

            elif trigger_type == "streaming_ready":
                # D√©clenchement par streaming direct
                return self._handle_new_raw_data(event)

            else:
                logger.warning(f"Type de trigger non reconnu: {trigger_type}")
                return {
                    'statusCode': 200,
                    'body': json.dumps({'message': 'Trigger ignor√©', 'type': trigger_type})
                }

        except Exception as e:
            logger.error(f"‚ùå Erreur orchestrateur: {e}")
            return {
                'statusCode': 500,
                'body': json.dumps({'error': str(e)})
            }

    def _identify_trigger(self, event: Dict) -> str:
        """Identifie le type de d√©clencheur"""

        # D√©clenchement direct depuis le processeur MPU en mode streaming (nouveau)
        if event.get('trigger_type') == 's3_streaming_ready':
            return "streaming_ready"

        # D√©clenchement direct depuis le processeur MPU (batch mode)
        if event.get('trigger_type') == 's3_batch_ready':
            return "batch_ready"

        # S3 Event (nouvelles donn√©es raw)
        if 'Records' in event and event['Records']:
            record = event['Records'][0]
            if record.get('eventSource') == 'aws:s3':
                s3_key = record['s3']['object']['key']
                if s3_key.startswith('raw/iot-measurements/') and 'mpu_christian' in s3_key.lower():
                    return "s3_raw"

        # EventBridge scheduled event
        if event.get('source') == 'aws.events' and event.get('detail-type') == 'Scheduled Event':
            return "schedule"

        # Manual trigger avec param√®tres
        if event.get('trigger_type'):
            return "manual"

        return "unknown"

    def _handle_new_raw_data(self, event: Dict) -> Dict:
        """Traite l'arriv√©e de nouvelles donn√©es MPU Christian dans S3 raw"""

        logger.info("üì¶ NOUVELLES DONN√âES MPU CHRISTIAN D√âTECT√âES")

        # V√©rifier si c'est un streaming direct haute fr√©quence
        if event.get('trigger_type') == 's3_streaming_ready':
            return self._handle_streaming_ready(event)

        # V√©rifier si c'est un batch haute fr√©quence
        if event.get('trigger_type') == 's3_batch_ready':
            return self._handle_batch_ready(event)

        # Traitement classique pour fichiers individuels via S3 events
        if 'Records' in event:
            record = event['Records'][0]
            bucket = record['s3']['bucket']['name']
            key = record['s3']['object']['key']

            logger.info(f"Fichier: s3://{bucket}/{key}")

            # V√©rifier si c'est bien des donn√©es MPU Christian
            if not self._is_mpu_christian_file(key):
                logger.info("Fichier ignor√© - pas de donn√©es MPU Christian")
                return {
                    'statusCode': 200,
                    'body': json.dumps({'message': 'Fichier ignor√©', 'file': key})
                }

            # D√©clencher la pipeline automatiquement
            return self._trigger_complete_pipeline(
                trigger_reason=f"Nouvelles donn√©es MPU: {key}",
                immediate=True
            )

        return {
            'statusCode': 400,
            'body': json.dumps({'message': 'Event format non reconnu'})
        }

    def _handle_streaming_ready(self, event: Dict) -> Dict:
        """Traite l'arriv√©e d'un fichier streaming direct haute fr√©quence"""

        logger.info("‚ö° STREAMING DIRECT MPU CHRISTIAN PR√äT")

        s3_key = event.get('s3_key', '')
        device_id = event.get('device_id', 'MPU_Christian_8266MOD')
        streaming_mode = event.get('streaming_mode', False)

        logger.info(f"Fichier streaming: {s3_key}")
        logger.info(f"Device: {device_id}")
        logger.info(f"Mode streaming direct: {streaming_mode}")

        # Pour le streaming direct, traitement diff√©r√© et optimis√©
        if streaming_mode:
            return self._trigger_complete_pipeline(
                trigger_reason=f"Streaming direct: {device_id}",
                immediate=False,  # Pas imm√©diat pour √©viter surcharge
                streaming_mode=True
            )
        else:
            return self._trigger_complete_pipeline(
                trigger_reason=f"Fichier MPU: {device_id}",
                immediate=True
            )

    def _handle_batch_ready(self, event: Dict) -> Dict:
        """Traite l'arriv√©e d'un batch haute fr√©quence"""

        logger.info("üîÑ BATCH HAUTE FR√âQUENCE MPU CHRISTIAN PR√äT")

        s3_key = event.get('s3_key', '')
        batch_size = event.get('batch_size', 0)
        high_frequency = event.get('high_frequency_mode', False)

        logger.info(f"Batch: {s3_key}")
        logger.info(f"Taille: {batch_size} √©chantillons")
        logger.info(f"Mode haute fr√©quence: {high_frequency}")

        # Pour les batches haute fr√©quence, traitement diff√©r√© et optimis√©
        if high_frequency:
            return self._trigger_complete_pipeline(
                trigger_reason=f"Batch haute fr√©quence: {batch_size} √©chantillons",
                immediate=False,  # Pas imm√©diat pour √©viter surcharge
                batch_mode=True
            )
        else:
            return self._trigger_complete_pipeline(
                trigger_reason=f"Batch MPU: {batch_size} √©chantillons",
                immediate=True
            )

    def _handle_scheduled_processing(self) -> Dict:
        """Traite le d√©clenchement programm√© (toutes les heures par exemple)"""

        logger.info("‚è∞ TRAITEMENT PROGRAMM√â D√âCLENCH√â")

        # V√©rifier s'il y a de nouvelles donn√©es √† traiter
        new_files = self._check_for_new_raw_files()

        if not new_files:
            logger.info("Aucune nouvelle donn√©e √† traiter")
            return {
                'statusCode': 200,
                'body': json.dumps({'message': 'Aucune nouvelle donn√©e'})
            }

        logger.info(f"Trouv√© {len(new_files)} nouveaux fichiers √† traiter")

        # D√©clencher la pipeline
        return self._trigger_complete_pipeline(
            trigger_reason=f"Traitement programm√©: {len(new_files)} fichiers",
            immediate=False
        )

    def _handle_manual_trigger(self, event: Dict) -> Dict:
        """Traite le d√©clenchement manuel"""

        logger.info("üë§ D√âCLENCHEMENT MANUEL")

        # Param√®tres optionnels
        params = event.get('parameters', {})
        force_reprocess = params.get('force_reprocess', False)

        return self._trigger_complete_pipeline(
            trigger_reason="D√©clenchement manuel",
            immediate=True,
            force_reprocess=force_reprocess
        )

    def _trigger_complete_pipeline(self, trigger_reason: str, immediate: bool = True, force_reprocess: bool = False) -> Dict:
        """D√©clenche la pipeline compl√®te raw‚Üíbronze‚Üísilver‚Üígold"""

        logger.info(f"üîÑ D√âCLENCHEMENT PIPELINE COMPL√àTE: {trigger_reason}")

        results = {
            'trigger_reason': trigger_reason,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'jobs_triggered': [],
            'errors': []
        }

        # Jobs Glue dans l'ordre (noms corrects des jobs existants)
        pipeline_jobs = [
            {
                'name': 'kidjamo-dev-raw-to-bronze',     # Nom correct du job existant
                'script': 'raw_to_bronze.py',
                'description': 'RAW ‚Üí BRONZE: Nettoyage et validation'
            },
            {
                'name': 'kidjamo-dev-bronze-to-silver',  # Nom correct du job existant
                'script': 'bronze_to_silver.py',
                'description': 'BRONZE ‚Üí SILVER: Enrichissement et transformation'
            },
            {
                'name': 'kidjamo-dev-silver-to-gold',    # Nom correct du job existant
                'script': 'silver_to_gold.py',
                'description': 'SILVER ‚Üí GOLD: Agr√©gation et analytics'
            }
        ]

        # D√©clencher chaque job dans l'ordre avec d√©lais
        for i, job_config in enumerate(pipeline_jobs):
            try:
                logger.info(f"üöÄ D√©clenchement job {i+1}/3: {job_config['description']}")

                # Param√®tres du job adapt√©s √† vos scripts Glue
                job_params = {
                    '--trigger-reason': trigger_reason,
                    '--immediate': str(immediate),
                    '--force-reprocess': str(force_reprocess),
                    '--bucket': self.bucket_name,
                    '--region': self.region
                }

                # Ajouter les param√®tres sp√©cifiques selon le job
                if job_config['name'] == 'kidjamo-dev-raw-to-bronze':
                    job_params.update({

                # D√©marrer le job Glue
                response = self.glue_client.start_job_run(
                    JobName=job_config['name'],
                    Arguments=job_params
                )

                job_run_id = response['JobRunId']
                results['jobs_triggered'].append({
                    'job_name': job_config['name'],
                    'job_run_id': job_run_id,
                    'script': job_config['script'],
                    'description': job_config['description'],
                    'status': 'STARTED'
                })

                logger.info(f"‚úÖ Job {job_config['name']} d√©marr√©: {job_run_id}")

                # D√©lai entre jobs pour √©viter les conflits (sauf pour le dernier)
                if i < len(pipeline_jobs) - 1 and immediate:
                    import time
                    time.sleep(30)  # 30 secondes entre chaque job

            except Exception as e:
                logger.error(f"‚ùå Erreur job {job_config['name']}: {e}")
                results['errors'].append({
                    'job_name': job_config['name'],
                    'script': job_config['script'],
                    'error': str(e)
                })

        # D√©clencher aussi le moteur d'alertes offline si n√©cessaire
        if len(results['jobs_triggered']) > 0:
            try:
                logger.info("üö® D√©clenchement moteur d'alertes offline...")

                alerts_response = self.glue_client.start_job_run(
                    JobName='kidjamo-offline-alerts-job',
                    Arguments={
                        '--trigger-reason': f"Post-pipeline: {trigger_reason}",
                        '--bucket': self.bucket_name
                    }
                )

                results['jobs_triggered'].append({
                    'job_name': 'kidjamo-offline-alerts-job',
                    'job_run_id': alerts_response['JobRunId'],
                    'script': 'offline_alerts_engine.py',
                    'description': 'Moteur d\'alertes offline',
                    'status': 'STARTED'
                })

                logger.info(f"‚úÖ Moteur alertes d√©marr√©: {alerts_response['JobRunId']}")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Moteur alertes non disponible: {e}")

        # Envoyer notification des r√©sultats
        self._send_notification(results)

        # Retour de la fonction Lambda
        status_code = 200 if not results['errors'] else 207  # 207 = Partial Success

        logger.info(f"üìä Pipeline d√©clench√©e: {len(results['jobs_triggered'])} jobs, {len(results['errors'])} erreurs")

        return {
            'statusCode': status_code,
            'body': json.dumps(results, indent=2)
        }

    def _is_mpu_christian_file(self, s3_key: str) -> bool:
        """V√©rifie si le fichier S3 contient des donn√©es MPU Christian"""

        key_lower = s3_key.lower()

        # V√©rifier les patterns de nommage
        mpu_patterns = ['mpu_christian', 'christian', '8266mod', 'mpu6050']

        return any(pattern in key_lower for pattern in mpu_patterns)

    def _check_for_new_raw_files(self, hours_back: int = 1) -> list:
        """V√©rifie s'il y a de nouveaux fichiers raw MPU Christian"""

        try:
            # Lister les fichiers raw r√©cents
            from datetime import timedelta
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)

            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix='raw/iot-measurements/'
            )

            new_files = []

            if 'Contents' in response:
                for obj in response['Contents']:
                    # V√©rifier si c'est un fichier MPU Christian r√©cent
                    if (self._is_mpu_christian_file(obj['Key']) and
                        obj['LastModified'] > cutoff_time):
                        new_files.append(obj)

            return new_files

        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification fichiers: {e}")
            return []

    def _send_notification(self, results: Dict):
        """Envoie une notification des r√©sultats"""

        try:
            # Vous pouvez configurer un topic SNS pour les notifications
            # topic_arn = "arn:aws:sns:eu-west-1:123456789012:kidjamo-pipeline-notifications"

            message = f"""
üîÑ Pipeline MPU Christian ex√©cut√©e

D√©clencheur: {results['trigger_reason']}
Timestamp: {results['timestamp']}

Jobs d√©marr√©s: {len(results['jobs_triggered'])}
Erreurs: {len(results['errors'])}

D√©tails:
{json.dumps(results, indent=2)}
            """

            logger.info("üìß Notification pr√©par√©e (SNS non configur√©)")
            # self.sns_client.publish(TopicArn=topic_arn, Message=message)

        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur notification: {e}")

# Instance globale pour AWS Lambda
orchestrator = AutoPipelineOrchestrator()

def lambda_handler(event, context):
    """Point d'entr√©e AWS Lambda"""
    return orchestrator.lambda_handler(event, context)
