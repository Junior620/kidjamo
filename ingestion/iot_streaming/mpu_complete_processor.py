#!/usr/bin/env python3
"""
Processeur Complet MPU Christian
- Intercepte les donn√©es Kinesis en temps r√©el
- D√©clenche les alertes critiques 
- Achemine vers S3 pour stockage raw
"""

import json
import boto3
import asyncio
import logging
from datetime import datetime, timezone
import time
import os
from typing import Dict, Any, List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MPUDataProcessor:
    """Processeur complet pour donn√©es MPU Christian"""

    def __init__(self):
        self.region = "eu-west-1"
        
        # Configuration Kinesis
        self.streams = [
            "kidjamo-alerts-stream",
            "kidjamo-vital-signs-stream", 
            "kidjamo-iot-stream"
        ]
        
        # Configuration S3
        self.s3_bucket = "kidjamo-dev-datalake-e75d5213"
        self.s3_raw_prefix = "raw/iot-measurements"

        # Configuration alertes
        self.alert_thresholds = {
            "acceleration_max": 20.0,
            "gyro_max": 500.0,
            "temperature_min": 10.0,
            "temperature_max": 50.0,
        }

        # NOUVEAU: Configuration pour flux haute fr√©quence
        self.batch_config = {
            "batch_size": 60,  # 60 √©chantillons = 1 minute de donn√©es
            "batch_timeout": 300,  # 5 minutes maximum avant for√ßage
            "pipeline_cooldown": 300,  # 5 minutes entre d√©clenchements pipeline (au lieu de 30 min)
            "alert_buffer_size": 10,  # Buffer pour √©viter spam d'alertes
        }

        # Buffers pour batching intelligent
        self.data_buffer = []
        self.last_pipeline_trigger = 0
        self.last_batch_save = time.time()
        self.alert_buffer = []

        # Clients AWS
        self.kinesis_client = boto3.client('kinesis', region_name=self.region)
        self.s3_client = boto3.client('s3', region_name=self.region)
        self.sns_client = boto3.client('sns', region_name=self.region)
        
        # M√©triques
        self.processed_count = 0
        self.alerts_triggered = 0
        self.s3_uploads = 0
        self.batches_saved = 0
        self.shard_iterators = {}
        
        logger.info("üöÄ Processeur MPU Christian initialis√© (MODE HAUTE FR√âQUENCE)")
        logger.info(f"üåä Streams surveill√©s: {self.streams}")
        logger.info(f"üì¶ Bucket S3: {self.s3_bucket}")
        logger.info(f"‚ö° Batch size: {self.batch_config['batch_size']} √©chantillons")
        logger.info(f"‚è±Ô∏è Pipeline cooldown: {self.batch_config['pipeline_cooldown']}s")

    async def start_processing(self):
        """D√©marre le traitement complet des donn√©es"""
        
        logger.info("üéØ D√âMARRAGE PROCESSEUR COMPLET MPU CHRISTIAN")
        logger.info("üì° Interception ‚Üí Alertes ‚Üí S3")
        logger.info("=" * 70)
        
        # Initialiser les streams Kinesis
        await self._initialize_kinesis_streams()
        
        # V√©rifier la connectivit√© S3
        await self._verify_s3_access()
        
        while True:
            try:
                # Traitement principal
                await self._process_kinesis_data()
                
                # Afficher les m√©triques toutes les 30 secondes
                if self.processed_count % 15 == 0 and self.processed_count > 0:
                    await self._display_metrics()
                
                await asyncio.sleep(2)  # V√©rification toutes les 2 secondes
                
            except KeyboardInterrupt:
                logger.info("üîÑ Arr√™t processeur demand√©")
                break
            except Exception as e:
                logger.error(f"‚ùå Erreur processeur: {e}")
                await asyncio.sleep(5)

    async def _initialize_kinesis_streams(self):
        """Initialise les streams Kinesis"""
        
        for stream_name in self.streams:
            try:
                response = self.kinesis_client.describe_stream(StreamName=stream_name)
                shards = response['StreamDescription']['Shards']
                
                self.shard_iterators[stream_name] = {}
                
                for shard in shards:
                    shard_id = shard['ShardId']
                    
                    # It√©rateur pour nouvelles donn√©es uniquement
                    iterator_response = self.kinesis_client.get_shard_iterator(
                        StreamName=stream_name,
                        ShardId=shard_id,
                        ShardIteratorType='LATEST'
                    )
                    
                    self.shard_iterators[stream_name][shard_id] = iterator_response['ShardIterator']
                    
                logger.info(f"‚úÖ {stream_name}: {len(shards)} shard(s) initialis√©s")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Stream {stream_name}: {e}")

    async def _verify_s3_access(self):
        """V√©rifie l'acc√®s au bucket S3"""
        
        try:
            # Tester l'acc√®s au bucket
            self.s3_client.head_bucket(Bucket=self.s3_bucket)
            logger.info(f"‚úÖ Acc√®s S3 v√©rifi√©: {self.s3_bucket}")
            
            # Cr√©er le pr√©fixe raw si n√©cessaire
            test_key = f"{self.s3_raw_prefix}/_test_access.json"
            test_data = {
                "test": True,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "processor": "MPUDataProcessor"
            }
            
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=test_key,
                Body=json.dumps(test_data),
                ContentType='application/json'
            )
            
            logger.info(f"‚úÖ √âcriture S3 test√©e: {self.s3_raw_prefix}/")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur acc√®s S3: {e}")
            raise

    async def _process_kinesis_data(self):
        """Traite les donn√©es Kinesis en temps r√©el avec surveillance am√©lior√©e"""

        for stream_name in self.streams:
            if stream_name not in self.shard_iterators:
                continue
                
            for shard_id, iterator in self.shard_iterators[stream_name].items():
                try:
                    # Lire plus de records pour capturer plus de donn√©es
                    response = self.kinesis_client.get_records(
                        ShardIterator=iterator,
                        Limit=100  # Augment√© de 50 √† 100 pour capturer plus de donn√©es
                    )
                    
                    records = response.get('Records', [])
                    next_iterator = response.get('NextShardIterator')
                    
                    # Mettre √† jour l'it√©rateur
                    if next_iterator:
                        self.shard_iterators[stream_name][shard_id] = next_iterator
                    
                    # Traiter chaque record
                    for record in records:
                        await self._process_single_record(stream_name, record)

                    # Log de surveillance pour debugging
                    if records:
                        logger.debug(f"üìä {stream_name}/{shard_id}: {len(records)} records trait√©s")

                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erreur lecture {stream_name}/{shard_id}: {e}")

    async def _process_single_record(self, stream_name: str, record: Dict):
        """Traite un record Kinesis individuel avec streaming direct vers S3"""

        try:
            # D√©coder les donn√©es
            data_bytes = record['Data']
            if isinstance(data_bytes, bytes):
                data_str = data_bytes.decode('utf-8')
            else:
                data_str = str(data_bytes)
            
            data = json.loads(data_str)
            self.processed_count += 1
            
            # Identifier si c'est du MPU Christian
            is_mpu_christian = self._is_mpu_christian_data(data)
            
            if is_mpu_christian:
                # 1. Analyser et d√©clencher alertes (imm√©diat pour s√©curit√©)
                alerts = await self._analyze_and_alert(data)
                
                # 2. NOUVEAU: Streaming direct vers S3 (pas de batching)
                s3_key = await self._stream_direct_to_s3(data, stream_name)

                # Log pour chaque √©chantillon en streaming
                logger.info(f"üéØ MPU Christian #{self.processed_count}: streaming direct ‚Üí {s3_key}")
                if alerts:
                    logger.info(f"   üö® {len(alerts)} alertes d√©clench√©es")

            else:
                # Afficher les donn√©es non-MPU pour debugging
                logger.debug(f"üìã Donn√©es autres (#{self.processed_count}): {stream_name}")
                
        except json.JSONDecodeError:
            logger.debug(f"‚ö†Ô∏è Record non-JSON dans {stream_name}")
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement record: {e}")

    async def _stream_direct_to_s3(self, data: Dict, stream_name: str) -> str:
        """Streaming direct vers S3 - chaque donn√©e bracelet devient imm√©diatement un fichier S3"""

        try:
            # G√©n√©rer la cl√© S3 unique pour chaque √©chantillon
            now = datetime.now(timezone.utc)

            # Cl√© S3 avec timestamp pr√©cis (microseconde) pour √©viter collisions
            s3_key = f"{self.s3_raw_prefix}/year={now.year}/month={now.month:02d}/day={now.day:02d}/hour={now.hour:02d}/mpu_christian_stream_{now.strftime('%Y%m%d_%H%M%S_%f')}.json"

            # Enrichir les donn√©es avec m√©tadonn√©es pour streaming
            enriched_data = {
                'original_data': data,
                'metadata': {
                    'ingestion_timestamp': now.isoformat(),
                    'stream_name': stream_name,
                    'processor': 'MPUDataProcessor_DirectStreaming',
                    'device_type': 'MPU_Christian_8266MOD',
                    'streaming_mode': 'direct_1hz',
                    'sample_id': f"sample_{now.strftime('%Y%m%d_%H%M%S_%f')}"
                }
            }

            # Upload imm√©diat vers S3
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=s3_key,
                Body=json.dumps(enriched_data, indent=2),
                ContentType='application/json',
                Metadata={
                    'device_id': str(data.get('device_id', 'MPU_Christian_8266MOD')),
                    'ingestion_time': now.isoformat(),
                    'stream': stream_name,
                    'streaming_mode': 'direct'
                }
            )

            self.s3_uploads += 1

            # D√©clencher la pipeline intelligemment (avec cooldown pour √©viter surcharge)
            await self._trigger_smart_pipeline_streaming(s3_key, data)

            return s3_key

        except Exception as e:
            logger.error(f"‚ùå Erreur streaming S3: {e}")
            return f"ERROR: {e}"

    async def _trigger_smart_pipeline_streaming(self, s3_key: str, data: Dict):
        """D√©clenche la pipeline intelligemment pour streaming haute fr√©quence"""

        current_time = time.time()

        # Cooldown plus intelligent pour streaming (5 minutes)
        if (current_time - self.last_pipeline_trigger) < self.batch_config['pipeline_cooldown']:
            logger.debug(f"üîÑ Pipeline en cooldown, fichier sauv√© sans d√©clenchement imm√©diat")
            return

        try:
            # Initialiser le client Lambda si pas d√©j√† fait
            if not hasattr(self, 'lambda_client'):
                self.lambda_client = boto3.client('lambda', region_name=self.region)

            # Pr√©parer l'√©v√©nement pour la Lambda d'orchestration
            event_payload = {
                "trigger_type": "s3_streaming_ready",
                "s3_bucket": self.s3_bucket,
                "s3_key": s3_key,
                "device_id": data.get('device_id', 'MPU_Christian_8266MOD'),
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "streaming_mode": True,
                "immediate_processing": False  # Pas imm√©diat pour √©viter surcharge
            }

            # D√©clencher la Lambda d'orchestration
            self.lambda_client.invoke(
                FunctionName='kidjamo-auto-pipeline-orchestrator',
                InvocationType='Event',
                Payload=json.dumps(event_payload)
            )

            self.last_pipeline_trigger = current_time
            logger.info(f"üöÄ Pipeline d√©clench√©e pour streaming (cooldown: {self.batch_config['pipeline_cooldown']}s)")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur d√©clenchement pipeline streaming: {e}")

    def _is_mpu_christian_data(self, data: Dict) -> bool:
        """Identifie si les donn√©es proviennent du MPU Christian - VERSION √âLARGIE"""

        # Rechercher dans TOUS les champs possibles (version plus permissive)
        search_fields = [
            str(data.get('device_id', '')),
            str(data.get('topic', '')),
            str(data.get('clientId', '')),
            str(data.get('deviceId', '')),
            str(data.get('source', '')),
            str(data.get('client_id', '')),  # Variante snake_case
            str(data.get('device', '')),
            str(data.get('sender', '')),
        ]

        # Mots-cl√©s √©largis pour capturer plus de variantes
        keywords = [
            'mpu_christian', 'christian', '8266mod', 'mpu6050',
            'bracelet_christian', 'christian_bracelet', 'mpu_8266',
            'esp8266', 'bracelet', 'wearable'
        ]
        
        for field in search_fields:
            field_str = str(field).lower()
            if any(keyword in field_str for keyword in keywords):
                logger.info(f"üéØ MPU Christian identifi√© par champ '{field}' = '{field_str}'")
                return True
        
        # V√©rifier aussi la structure des donn√©es (acc√©l√©rom√®tre/gyroscope) - plus permissif
        sensor_keys = [
            'accelerometer', 'gyroscope', 'accel', 'gyro',
            'ax', 'ay', 'az', 'gx', 'gy', 'gz',
            'acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z',
            'acceleration', 'rotation', 'motion'
        ]

        has_sensor_data = any(key in data for key in sensor_keys)

        if has_sensor_data:
            logger.info(f"üéØ MPU Christian identifi√© par donn√©es capteurs: {[k for k in sensor_keys if k in data]}")
            return True

        # Log pour debugging - voir toutes les donn√©es qui passent
        logger.debug(f"üìã Donn√©es non-MPU: device_id={data.get('device_id')}, clientId={data.get('clientId')}, keys={list(data.keys())}")

        return False

    async def _analyze_and_alert(self, data: Dict) -> List[Dict]:
        """Analyse les donn√©es et d√©clenche des alertes si n√©cessaire"""
        
        alerts = []
        
        # Analyser acc√©l√©rom√®tre
        if 'accelerometer' in data:
            accel = data['accelerometer']
            ax, ay, az = accel.get('x', 0), accel.get('y', 0), accel.get('z', 0)
            
            # Calculer l'acc√©l√©ration totale
            total_accel = (ax**2 + ay**2 + az**2)**0.5
            
            if total_accel > self.alert_thresholds['acceleration_max']:
                alert = {
                    'type': 'ACCELERATION_CRITIQUE',
                    'value': total_accel,
                    'threshold': self.alert_thresholds['acceleration_max'],
                    'severity': 'HIGH',
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'device': data.get('device_id', 'MPU_Christian_8266MOD')
                }
                alerts.append(alert)
                await self._trigger_alert(alert)
        
        # Analyser gyroscope
        if 'gyroscope' in data:
            gyro = data['gyroscope']
            gx, gy, gz = gyro.get('x', 0), gyro.get('y', 0), gyro.get('z', 0)
            
            # Calculer la rotation totale
            total_gyro = (gx**2 + gy**2 + gz**2)**0.5
            
            if total_gyro > self.alert_thresholds['gyro_max']:
                alert = {
                    'type': 'ROTATION_CRITIQUE',
                    'value': total_gyro,
                    'threshold': self.alert_thresholds['gyro_max'],
                    'severity': 'HIGH',
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'device': data.get('device_id', 'MPU_Christian_8266MOD')
                }
                alerts.append(alert)
                await self._trigger_alert(alert)
        
        # Analyser temp√©rature
        if 'temperature' in data:
            temp = data['temperature']
            
            if temp < self.alert_thresholds['temperature_min'] or temp > self.alert_thresholds['temperature_max']:
                alert = {
                    'type': 'TEMPERATURE_ANORMALE',
                    'value': temp,
                    'thresholds': [self.alert_thresholds['temperature_min'], self.alert_thresholds['temperature_max']],
                    'severity': 'MEDIUM',
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'device': data.get('device_id', 'MPU_Christian_8266MOD')
                }
                alerts.append(alert)
                await self._trigger_alert(alert)
        
        return alerts

    async def _trigger_alert(self, alert: Dict):
        """D√©clenche une alerte"""
        
        self.alerts_triggered += 1
        
        logger.warning(f"üö® ALERTE #{self.alerts_triggered}: {alert['type']}")
        logger.warning(f"   üìä Valeur: {alert['value']}")
        logger.warning(f"   ‚ö†Ô∏è Seuil: {alert.get('threshold', alert.get('thresholds'))}")
        logger.warning(f"   üì± Device: {alert['device']}")
        
        # Envoyer vers SNS (si configur√©)
        try:
            # Vous pouvez configurer un topic SNS pour les alertes
            pass
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur SNS: {e}")

    async def _display_metrics(self):
        """Affiche les m√©triques du processeur"""
        
        logger.info("üìä M√âTRIQUES PROCESSEUR:")
        logger.info(f"   üì® Records trait√©s: {self.processed_count}")
        logger.info(f"   üö® Alertes d√©clench√©es: {self.alerts_triggered}")
        logger.info(f"   üì¶ Uploads S3: {self.s3_uploads}")
        logger.info(f"   ‚è±Ô∏è Uptime: {time.time() - self.start_time:.0f}s")
        logger.info("=" * 70)

async def main():
    """Point d'entr√©e principal"""
    
    logger.info("üöÄ D√âMARRAGE PROCESSEUR COMPLET MPU CHRISTIAN")
    logger.info("üéØ Pipeline: Kinesis ‚Üí Alertes ‚Üí S3")
    logger.info("=" * 70)
    
    processor = MPUDataProcessor()
    processor.start_time = time.time()
    
    try:
        await processor.start_processing()
    except KeyboardInterrupt:
        logger.info("üõë Arr√™t processeur demand√© par l'utilisateur")
    except Exception as e:
        logger.error(f"‚ùå Erreur critique: {e}")
    finally:
        logger.info("üîÑ Processeur arr√™t√©")
        await processor._display_metrics()

if __name__ == "__main__":
    asyncio.run(main())
