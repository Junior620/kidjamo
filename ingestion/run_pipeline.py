# run_pipeline.py
"""
Script d'orchestration pour ex√©cuter l'ensemble du pipeline d'ingestion
Ordre d'ex√©cution: raw ‚Üí bronze ‚Üí silver ‚Üí gold
"""

import subprocess
import sys
import time
from datetime import datetime
import os

def log_message(message, level="INFO"):
    """Affiche un message avec horodatage"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {level}: {message}")

def run_spark_job(job_path, job_name, spark_command):
    """Ex√©cute un job Spark et retourne True si succ√®s, False sinon"""
    log_message(f"D√©marrage du job: {job_name}")
    start_time = time.time()

    try:
        # ‚úÖ EX√âCUTION DIRECTE AVEC PYTHON (plus fiable que spark-submit)
        result = subprocess.run(
            ["python", job_path],
            capture_output=True,
            text=True,
            check=True
        )

        end_time = time.time()
        duration = end_time - start_time

        log_message(f"‚úÖ {job_name} termin√© avec succ√®s en {duration:.1f}s")

        # Affichage des messages importants du job
        output_lines = result.stdout.split('\n')
        for line in output_lines:
            if line.strip() and any(keyword in line for keyword in
                ["SUCCESS", "INFO:", "STATS:", "ERROR:", "WARNING:", "lignes", "rows"]):
                print(f"    {line.strip()}")

        return True

    except subprocess.CalledProcessError as e:
        end_time = time.time()
        duration = end_time - start_time

        log_message(f"‚ùå {job_name} a √©chou√© apr√®s {duration:.1f}s", "ERROR")

        # Affichage des vraies erreurs
        print("Erreurs:")
        if e.stderr:
            stderr_lines = e.stderr.split('\n')
            for line in stderr_lines[-20:]:  # Derni√®res 20 lignes d'erreur
                if line.strip():
                    print(f"    {line.strip()}")

        if e.stdout:
            stdout_lines = e.stdout.split('\n')
            for line in stdout_lines[-10:]:  # Derni√®res 10 lignes de sortie
                if line.strip() and ("ERROR" in line or "Exception" in line or "Traceback" in line):
                    print(f"    {line.strip()}")

        return False

    except Exception as e:
        log_message(f"‚ùå Erreur inattendue pour {job_name}: {str(e)}", "ERROR")
        return False

def check_prerequisites():
    """V√©rifie que les pr√©requis sont pr√©sents"""
    log_message("V√©rification des pr√©requis...")

    # Chemins possibles pour Spark
    spark_paths = [
        "spark-submit",  # Si dans le PATH
        "C:\\spark-3.5.5-bin-hadoop3\\spark-3.5.5-bin-hadoop3\\bin\\spark-submit.cmd",
        "C:\\spark-3.5.5-bin-hadoop3\\bin\\spark-submit.cmd",
        "C:\\spark\\bin\\spark-submit.cmd"
    ]

    spark_command = None

    # V√©rifier que spark-submit est disponible
    for path in spark_paths:
        try:
            subprocess.run([path, "--version"],
                          capture_output=True, check=True, timeout=30)
            spark_command = path
            log_message(f"‚úÖ Spark trouv√© √†: {path}")
            break
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
            continue

    if spark_command is None:
        log_message("‚ùå Spark n'est pas disponible. V√©rifiez votre installation.", "ERROR")
        log_message("Chemins test√©s:", "ERROR")
        for path in spark_paths:
            log_message(f"  - {path}", "ERROR")
        return False, None

    # V√©rifier que nous sommes dans le bon r√©pertoire
    if not os.path.exists("jobs"):
        log_message("‚ùå R√©pertoire 'jobs' non trouv√©. Ex√©cutez ce script depuis le dossier ingestion/", "ERROR")
        return False, None

    # V√©rifier que tous les jobs existent
    jobs = [
        "jobs/01_to_raw.py",
        "jobs/02_raw_to_bronze.py",
        "jobs/03_bronze_to_silver.py",
        "jobs/04_silver_to_gold.py"
    ]

    for job in jobs:
        if not os.path.exists(job):
            log_message(f"‚ùå Job manquant: {job}", "ERROR")
            return False, None

    log_message("‚úÖ Tous les pr√©requis sont satisfaits")
    return True, spark_command

def cleanup_old_data():
    """Nettoie les anciennes donn√©es (optionnel)"""
    log_message("Nettoyage des anciennes donn√©es...")

    # Suppression des dossiers de sortie (optionnel - d√©commenter si besoin)
    # directories_to_clean = ["raw", "bronze", "silver", "gold"]
    # for directory in directories_to_clean:
    #     if os.path.exists(directory):
    #         import shutil
    #         shutil.rmtree(directory)
    #         log_message(f"Supprim√©: {directory}")

    log_message("‚úÖ Nettoyage termin√©")

def check_data_availability():
    """V√©rifie que les donn√©es sources sont disponibles"""
    log_message("V√©rification de la disponibilit√© des donn√©es...")

    # V√©rifier le r√©pertoire landing
    if os.path.exists("landing") and os.listdir("landing"):
        files = os.listdir("landing")
        log_message(f"‚úÖ Donn√©es trouv√©es dans landing: {len(files)} fichier(s)")
        for file in files[:5]:  # Afficher les 5 premiers
            log_message(f"    - {file}")
        return True
    else:
        log_message("‚ö†Ô∏è  Aucune donn√©e trouv√©e dans le r√©pertoire landing", "WARNING")
        return False

def main():
    """Fonction principale d'orchestration"""
    log_message("üöÄ D√âMARRAGE DU PIPELINE D'INGESTION COMPLET")
    log_message("=" * 60)

    # V√©rifications pr√©liminaires
    prereq_ok, spark_command = check_prerequisites()
    if not prereq_ok:
        log_message("‚ùå Les pr√©requis ne sont pas satisfaits. Arr√™t du pipeline.", "ERROR")
        sys.exit(1)

    check_data_availability()

    # D√©finition des jobs dans l'ordre d'ex√©cution
    pipeline_jobs = [
        {
            "path": "jobs/01_to_raw.py",
            "name": "01 - Landing to Raw",
            "description": "Ingestion des donn√©es depuis landing vers raw"
        },
        {
            "path": "jobs/02_raw_to_bronze.py",
            "name": "02 - Raw to Bronze",
            "description": "Transformation et nettoyage vers bronze"
        },
        {
            "path": "jobs/03_bronze_to_silver.py",
            "name": "03 - Bronze to Silver",
            "description": "Enrichissement et validation vers silver"
        },  # ‚úÖ R√âACTIV√â - L'√©tape 3 fonctionne maintenant parfaitement !
        {
            "path": "jobs/04_silver_to_gold.py",
            "name": "04 - Silver to Gold",
            "description": "Agr√©gations et features ML vers gold"
        }
    ]

    # Ex√©cution s√©quentielle des jobs
    pipeline_start_time = time.time()
    successful_jobs = 0
    failed_jobs = 0

    for i, job in enumerate(pipeline_jobs, 1):
        log_message("=" * 60)
        log_message(f"√âTAPE {i}/4: {job['name']}")
        log_message(f"Description: {job['description']}")
        log_message("-" * 40)

        success = run_spark_job(job["path"], job["name"], spark_command)

        if success:
            successful_jobs += 1
            log_message(f"‚úÖ √©tape {i} termin√©e avec succ√®s")
        else:
            failed_jobs += 1
            log_message(f"‚ùå √âtape {i} a √©chou√©", "ERROR")

            # Demander si on continue malgr√© l'erreur
            response = input(f"\n√âtape {i} a √©chou√©. Continuer avec l'√©tape suivante ? (y/N): ")
            if response.lower() != 'y':
                log_message("Pipeline interrompu par l'utilisateur", "WARNING")
                break

        # Pause entre les jobs
        if i < len(pipeline_jobs):
            log_message("Pause de 2 secondes avant l'√©tape suivante...")
            time.sleep(2)

    # R√©sum√© final
    pipeline_end_time = time.time()
    total_duration = pipeline_end_time - pipeline_start_time

    log_message("=" * 60)
    log_message("üèÅ R√âSUM√â DU PIPELINE")
    log_message(f"Dur√©e totale: {total_duration:.1f} secondes")
    log_message(f"Jobs r√©ussis: {successful_jobs}")
    log_message(f"Jobs √©chou√©s: {failed_jobs}")

    if failed_jobs == 0:
        log_message("PIPELINE TERMIN√â AVEC SUCC√àS !")
        log_message("Donn√©es disponibles dans:")
        log_message("  - raw/     : Donn√©es brutes")
        log_message("  - bronze/  : Donn√©es nettoy√©es")
        log_message("  - silver/  : Donn√©es enrichies")
        log_message("  - gold/    : Agr√©gations et features ML")
    else:
        log_message(f"‚ö†Ô∏è  Pipeline termin√© avec {failed_jobs} erreur(s)", "WARNING")
        log_message("V√©rifiez les logs ci-dessus pour plus de d√©tails")

    log_message("=" * 60)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log_message("\nüõë Pipeline interrompu par l'utilisateur", "WARNING")
        sys.exit(1)
    except Exception as e:
        log_message(f"‚ùå Erreur inattendue: {str(e)}", "ERROR")
        sys.exit(1)
